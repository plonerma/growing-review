<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Review of Literature on Growing Neural Networks</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="https://fonts.xz.style/serve/inter.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css">
  <link rel="stylesheet" href="style.css">
</head>
<body>
<header class="section">
<div class="container">

<h1 class="title">Review of Literature on Growing Neural Networks</h1>


<div>

<div class="author-container">
<p class="author"><a href="https://maxploner.de">Max Ploner</a></p>
</div>
</div>

</div>
</header>



<p>This article aims to summarize the existing literature concerned with
growing artificial neural networks. For each paper it will list the most
significant contribution. The following four questions will guide the
summary of each paper:</p>
<ol type="1">
<li>Why are models grown? What is the goal or metric the approach is
evaluated on?</li>
<li>When are the models grown?</li>
<li>Where are the models grown?</li>
<li>How are the the new parts initialized?</li>
</ol>
<p>Each paper tries to make progress in answering at least one of the
questions. Hence, they can be used to categorize these papers.</p>
<h2
id="compnet-neural-networks-growing-via-the-compact-network-morphism-lucompnetneuralnetworks2018a"><a
href="http://arxiv.org/abs/1804.10316">CompNet: Neural Networks Growing
via the Compact Network Morphism</a> <span class="citation"
data-cites="luCompNetNeuralNetworks2018a">[@luCompNetNeuralNetworks2018a]</span></h2>
<p><em>coming soon</em></p>
<h2
id="nest-a-neural-network-synthesis-tool-based-on-a-grow-and-prune-paradigm-dainestneuralnetwork2018"><a
href="http://arxiv.org/abs/1711.02017">NeST: A Neural Network Synthesis
Tool Based on a Grow-and-Prune Paradigm</a> <span class="citation"
data-cites="daiNeSTNeuralNetwork2018">[@daiNeSTNeuralNetwork2018]</span></h2>
<p>some summary on prune an grow (nest)</p>
<h2
id="autogrow-automatic-layer-growing-in-deep-convolutional-networks-wenautogrowautomaticlayer2020a"><a
href="http://arxiv.org/abs/1906.02909">AutoGrow: Automatic Layer Growing
in Deep Convolutional Networks</a> <span class="citation"
data-cites="wenAutoGrowAutomaticLayer2020a">[@wenAutoGrowAutomaticLayer2020a]</span></h2>
<p><em>coming soon</em></p>
<h2
id="gradmax-growing-neural-networks-using-gradient-information-evcigradmaxgrowingneural2022a"><a
href="http://arxiv.org/abs/2201.05125">GradMax: Growing Neural Networks
Using Gradient Information</a> <span class="citation"
data-cites="evciGradMaxGrowingNeural2022a">[@evciGradMaxGrowingNeural2022a]</span></h2>
<p>grad max summary</p>
</body>
</html>
