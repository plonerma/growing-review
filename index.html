<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Review of Literature on Growing Neural Networks</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="https://fonts.xz.style/serve/inter.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css">
  <link rel="stylesheet" href="style.css">

<script src="https://kit.fontawesome.com/81529ac3d0.js" crossorigin="anonymous"></script>

</head>
<body>
<header class="section">

<div class="container" style="float: right;">
<a href="growing_review.pdf">
  <button class="pdf-button"><i class="far fa-file-pdf"></i> View as PDF</button>
</a>

<a href="\#">
  <button class="github-button"><i class="fab fa-github"></i> Edit on Github</button>
</a>
</div>
<div class="container">


<h1 class="title">Review of Literature on Growing Neural Networks</h1>


<div>

<div class="author-container">
<p class="author"><a href="https://maxploner.de">Max Ploner</a></p>
<p class="affilation"><a href="https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en">HU
Berlin</a></p>
<p class="affilation"><a href="https://www.scienceofintelligence.de/">Science
of Intelligence</a></p>
</div>
</div>

</div>
</header>



<h2 id="introduction">Introduction</h2>
<p>This article aims to summarize the existing literature concerned with
growing artificial neural networks. For each paper it will list the most
significant contribution. The following four questions will guide the
summary of each paper:</p>
<ol type="1">
<li>Why are models grown? What is the goal or metric the approach is
evaluated on?</li>
<li>When are the models grown?</li>
<li>Where are the models grown?</li>
<li>How are the the new parts initialized?</li>
</ol>
<p>Each paper tries to make progress in answering at least one of the
questions. Hence, they can be used to categorize these papers.</p>
<h2
id="compnet-neural-networks-growing-via-the-compact-network-morphism-lucompnetneuralnetworks2018a"><a
href="http://arxiv.org/abs/1804.10316">CompNet: Neural Networks Growing
via the Compact Network Morphism</a> <span class="citation"
data-cites="luCompNetNeuralNetworks2018a">(<a
href="#ref-luCompNetNeuralNetworks2018a" role="doc-biblioref">Lu, Ma,
and Faltings 2018</a>)</span></h2>
<p><em>coming soon</em></p>
<h2
id="nest-a-neural-network-synthesis-tool-based-on-a-grow-and-prune-paradigm-dainestneuralnetwork2018"><a
href="http://arxiv.org/abs/1711.02017">NeST: A Neural Network Synthesis
Tool Based on a Grow-and-Prune Paradigm</a> <span class="citation"
data-cites="daiNeSTNeuralNetwork2018">(<a
href="#ref-daiNeSTNeuralNetwork2018" role="doc-biblioref">Dai, Yin, and
Jha 2018</a>)</span></h2>
<p><em>coming soon</em></p>
<h2
id="autogrow-automatic-layer-growing-in-deep-convolutional-networks-wenautogrowautomaticlayer2020a"><a
href="http://arxiv.org/abs/1906.02909">AutoGrow: Automatic Layer Growing
in Deep Convolutional Networks</a> <span class="citation"
data-cites="wenAutoGrowAutomaticLayer2020a">(<a
href="#ref-wenAutoGrowAutomaticLayer2020a" role="doc-biblioref">Wen et
al. 2020</a>)</span></h2>
<p><em>coming soon</em></p>
<h2
id="gradmax-growing-neural-networks-using-gradient-information-evcigradmaxgrowingneural2022a"><a
href="http://arxiv.org/abs/2201.05125">GradMax: Growing Neural Networks
Using Gradient Information</a> <span class="citation"
data-cites="evciGradMaxGrowingNeural2022a">(<a
href="#ref-evciGradMaxGrowingNeural2022a" role="doc-biblioref">Evci et
al. 2022</a>)</span></h2>
<p><em>coming soon</em></p>
<h1 class="unnumbered" id="bibliography">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-daiNeSTNeuralNetwork2018" class="csl-entry"
role="listitem">
Dai, Xiaoliang, Hongxu Yin, and Niraj K. Jha. 2018.
<span>“<span>NeST</span>: <span>A Neural Network Synthesis Tool
Based</span> on a <span class="nocase">Grow-and-Prune
Paradigm</span>,”</span> no. arXiv:1711.02017 (June). <a
href="https://doi.org/10.48550/arXiv.1711.02017">https://doi.org/10.48550/arXiv.1711.02017</a>.
</div>
<div id="ref-evciGradMaxGrowingNeural2022a" class="csl-entry"
role="listitem">
Evci, Utku, Bart van Merriënboer, Thomas Unterthiner, Max Vladymyrov,
and Fabian Pedregosa. 2022. <span>“<span>GradMax</span>: <span>Growing
Neural Networks</span> Using <span>Gradient Information</span>.”</span>
<span>arXiv</span>. <a
href="http://arxiv.org/abs/2201.05125">http://arxiv.org/abs/2201.05125</a>.
</div>
<div id="ref-luCompNetNeuralNetworks2018a" class="csl-entry"
role="listitem">
Lu, Jun, Wei Ma, and Boi Faltings. 2018. <span>“<span>CompNet</span>:
<span>Neural</span> Networks Growing via the Compact Network
Morphism.”</span> <span>arXiv</span>. <a
href="https://doi.org/10.48550/arXiv.1804.10316">https://doi.org/10.48550/arXiv.1804.10316</a>.
</div>
<div id="ref-wenAutoGrowAutomaticLayer2020a" class="csl-entry"
role="listitem">
Wen, Wei, Feng Yan, Yiran Chen, and Hai Li. 2020.
<span>“<span>AutoGrow</span>: <span>Automatic Layer Growing</span> in
<span>Deep Convolutional Networks</span>.”</span> <span>arXiv</span>. <a
href="https://doi.org/10.48550/arXiv.1906.02909">https://doi.org/10.48550/arXiv.1906.02909</a>.
</div>
</div>
</body>
</html>
