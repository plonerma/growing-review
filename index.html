<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Review of Literature on Growing Neural Networks</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="https://fonts.xz.style/serve/inter.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css">
  <link rel="stylesheet" href="style.css">
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>

<script src="https://kit.fontawesome.com/81529ac3d0.js" crossorigin="anonymous"></script>
<script src="sortable_table.js" crossorigin="anonymous"></script>

</head>
<body>
<header class="section">

<div class="container" style="float: right;">
<a href="growing_review.pdf">
  <button class="pdf-button"><i class="far fa-file-pdf"></i> View as PDF</button>
</a>

<a href="https://github.com/plonerma/growing-review">
  <button class="github-button"><i class="fab fa-github"></i> Edit on Github</button>
</a>
</div>
<div class="container">


<h1 class="title">Review of Literature on Growing Neural Networks</h1>


<div>

<div class="author-container">
<p class="author"><a href="https://maxploner.de">Max Ploner</a></p>
<p class="affilation"><a href="https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en">HU
Berlin</a></p>
<p class="affilation"><a href="https://www.scienceofintelligence.de/">Science
of Intelligence</a></p>
</div>
</div>

</div>
</header>


<nav id="TOC">
  <input id="toc-collapsible" class="toc-toggle" type="checkbox">
  <label for="toc-collapsible" class="toc-toggle-label">Contents</label>
  <div class="toc-content">
    <div class="toc-content-inner">
<ul>
<li><a href="#sec:introduction"
id="toc-sec:introduction">Introduction</a>
<ul>
<li><a href="#topics-of-interest" id="toc-topics-of-interest">Topics of
Interest</a></li>
</ul></li>
<li><a href="#sec:categorization"
id="toc-sec:categorization">Categorization</a></li>
<li><a href="#sec:summaries" id="toc-sec:summaries">Summaries of the
Reviewed Publications</a>
<ul>
<li><a href="#sec:chenNet2NetAcceleratingLearning2016"
id="toc-sec:chenNet2NetAcceleratingLearning2016">Net2Net: Accelerating
Learning via Knowledge Transfer <span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">(Chen, Goodfellow, and
Shlens 2016)</span></a></li>
<li><a href="#sec:weiNetworkMorphism2016"
id="toc-sec:weiNetworkMorphism2016">Network Morphism <span
class="citation" data-cites="weiNetworkMorphism2016">(Wei et al.
2016)</span></a></li>
<li><a href="#sec:rusuProgressiveNeuralNetworks2016"
id="toc-sec:rusuProgressiveNeuralNetworks2016">Progressive Neural
Networks <span class="citation"
data-cites="rusuProgressiveNeuralNetworks2016">(Rusu et al.
2016)</span></a></li>
<li><a href="#sec:caiEfficientArchitectureSearch2017"
id="toc-sec:caiEfficientArchitectureSearch2017">Efficient Architecture
Search by Network Transformation <span class="citation"
data-cites="caiEfficientArchitectureSearch2017">(Cai et al.
2017)</span></a></li>
<li><a href="#sec:elskenSimpleEfficientArchitecture2017"
id="toc-sec:elskenSimpleEfficientArchitecture2017">Simple And Efficient
Architecture Search for Convolutional Neural Networks <span
class="citation"
data-cites="elskenSimpleEfficientArchitecture2017">(Elsken, Metzen, and
Hutter 2017)</span></a></li>
<li><a href="#sec:daiNeSTNeuralNetwork2018"
id="toc-sec:daiNeSTNeuralNetwork2018">NeST: A Neural Network Synthesis
Tool Based on a Grow-and-Prune Paradigm <span class="citation"
data-cites="daiNeSTNeuralNetwork2018">(Dai, Yin, and Jha
2018)</span></a></li>
<li><a href="#sec:caiPathLevelNetworkTransformation2018"
id="toc-sec:caiPathLevelNetworkTransformation2018">Path-Level Network
Transformation for Efficient Architecture Search <span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">(Cai et al.
2018)</span></a></li>
<li><a href="#sec:hungCompactingPickingGrowing2019"
id="toc-sec:hungCompactingPickingGrowing2019">Compacting, Picking and
Growing for Unforgetting Continual Learning <span class="citation"
data-cites="hungCompactingPickingGrowing2019">(Hung et al.
2019)</span></a></li>
<li><a href="#sec:gongEfficientTrainingBERT2019"
id="toc-sec:gongEfficientTrainingBERT2019">Efficient Training of BERT by
Progressively Stacking <span class="citation"
data-cites="gongEfficientTrainingBERT2019">(Gong et al.
2019)</span></a></li>
<li><a href="#sec:wuFirefly2021" id="toc-sec:wuFirefly2021">Firefly
Neural Architecture Descent: A General Approach for Growing Neural
Networks <span class="citation" data-cites="wuFirefly2021">(Wu et al.
2020)</span></a></li>
<li><a href="#sec:hassantabarSCANNSynthesisCompact2021"
id="toc-sec:hassantabarSCANNSynthesisCompact2021">SCANN: Synthesis of
Compact and Accurate Neural Networks <span class="citation"
data-cites="hassantabarSCANNSynthesisCompact2021">(Hassantabar, Wang,
and Jha 2021)</span></a></li>
<li><a href="#sec:evciGradMaxGrowingNeural2022"
id="toc-sec:evciGradMaxGrowingNeural2022">GradMax: Growing Neural
Networks Using Gradient Information <span class="citation"
data-cites="evciGradMaxGrowingNeural2022">(Evci et al.
2022)</span></a></li>
</ul></li>
<li><a href="#sec:references"
id="toc-sec:references">References</a></li>
</ul>
    </div>
  </div>
</div>
</nav>


<button id="scroll-to-top" title="Scroll to the top of the page"><i class="fas fa-arrow-up"></i></button>




<h1 id="sec:introduction">Introduction</h1>
<p>This article aims to summarize the existing literature concerned with
growing artificial neural networks. For each paper it will list the most
significant contribution. The following four questions will guide the
summary of each paper:</p>
<ol type="1">
<li><strong>Why</strong> are models grown? What is the goal or metric
the approach is evaluated on?</li>
<li><strong>When</strong> are the models grown?</li>
<li><strong>Where</strong> are the models grown?</li>
<li><strong>How</strong> are the the new parts initialized?</li>
</ol>
<p>Each paper tries to make progress in answering at least one of the
questions. Hence, they can be used to categorize these papers.</p>
<h2 id="topics-of-interest">Topics of Interest</h2>
<p>This articles reviews publications on Artificial Neural Networks (ANN
or simply NN) which are grown in some way or another. It is focuses on
networks trained using backpropagation with gradient descent and
excludes research areas such as self-organizing maps <span
class="citation" data-cites="boineeAutomaticClassificationUsing2003">(<a
href="#ref-boineeAutomaticClassificationUsing2003"
role="doc-biblioref">Boinee, De Angelis, and Milotti 2003</a>)</span>,
growing neural gas <span class="citation"
data-cites="fritzkeGrowingNeuralGas1994">(GNG, <a
href="#ref-fritzkeGrowingNeuralGas1994" role="doc-biblioref">Fritzke
1994</a>)</span>, self-organizing networks <span class="citation"
data-cites="piastraGrowingSelfOrganizingNetwork2009">(<a
href="#ref-piastraGrowingSelfOrganizingNetwork2009"
role="doc-biblioref">Piastra 2009</a>)</span>, and growing neural
forests <span class="citation"
data-cites="palomoLearningTopologiesGrowing2016">(<a
href="#ref-palomoLearningTopologiesGrowing2016"
role="doc-biblioref">Palomo and López-Rubio 2016</a>)</span>.</p>
<p>Additionally, we do not consider an ANN to be grown if simply another
classification head is introduced when a new task is encountered in an
continuous learning (CL) setting. Instead, in continuous learning
settings, we require the shared parts of the network to be grown.</p>
<h1 id="sec:categorization">Categorization</h1>
<div class="sortable paper-categories">
<div id="tbl:papers_cat" class="tablenos">
<table id="tbl:papers_cat" style="width:100%;">
<caption><span>Table 1:</span> Papers according to the three questions
(see Section <a href="#sec:introduction">1</a>). </caption>
<colgroup>
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th>Short Title</th>
<th>Year</th>
<th>Why?</th>
<th>When?</th>
<th>Where?</th>
<th>How?</th>
<th>Paper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="#sec:chenNet2NetAcceleratingLearning2016">Net2Net</a></td>
<td>2016</td>
<td>Knowledge transfer for NAS(?), already mentions lifelong learning
(no experiments)</td>
<td>Single growth event</td>
<td>Not dynamic: Width is uniformly grown, new layers are added towards
the end</td>
<td>Function-preserving transforms (identity matrix)</td>
<td><span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:weiNetworkMorphism2016">Network Morphism</a></td>
<td>2016</td>
<td>Knowledge transfer</td>
<td>Single growth event</td>
<td>Not dynamic: Width is uniformly grown, new layers are added towards
the end</td>
<td>Function-preserving transform, less sparse init.</td>
<td><span class="citation" data-cites="weiNetworkMorphism2016">Wei et
al. (<a href="#ref-weiNetworkMorphism2016"
role="doc-biblioref">2016</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="#sec:rusuProgressiveNeuralNetworks2016">Progressive
Nets</a></td>
<td>2016</td>
<td>Continual Learning</td>
<td>On new tasks</td>
<td>New columns</td>
<td>Random init</td>
<td><span class="citation"
data-cites="rusuProgressiveNeuralNetworks2016">Rusu et al. (<a
href="#ref-rusuProgressiveNeuralNetworks2016"
role="doc-biblioref">2016</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:caiEfficientArchitectureSearch2017">NAS using Net
Transforms</a></td>
<td>2017</td>
<td>NAS</td>
<td>Fixed schedule</td>
<td>Decided by RL agent</td>
<td>Function-preserving transforms</td>
<td><span class="citation"
data-cites="caiEfficientArchitectureSearch2017">Cai et al. (<a
href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="#sec:elskenSimpleEfficientArchitecture2017">NASH</a></td>
<td>2017</td>
<td>NAS</td>
<td>Iterativly grow and train a set of networks, then pick the best</td>
<td>Randomly selected (multiple alternatives)</td>
<td>Function-preserving transforms</td>
<td><span class="citation"
data-cites="elskenSimpleEfficientArchitecture2017">Elsken, Metzen, and
Hutter (<a href="#ref-elskenSimpleEfficientArchitecture2017"
role="doc-biblioref">2017</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:daiNeSTNeuralNetwork2018">NeST</a></td>
<td>2018</td>
<td>NAS</td>
<td>Growth phase, then prune phase with iterative retraining in each
phase</td>
<td>Gradient-based selection</td>
<td>Initialization based on gradient</td>
<td><span class="citation" data-cites="daiNeSTNeuralNetwork2018">Dai,
Yin, and Jha (<a href="#ref-daiNeSTNeuralNetwork2018"
role="doc-biblioref">2018</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="#sec:caiPathLevelNetworkTransformation2018">Path-Level
Transformations</a></td>
<td>2018</td>
<td>NAS</td>
<td>Fixed schedule</td>
<td>Decided by RL agent</td>
<td>Function-preserving transforms</td>
<td><span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">Cai et al. (<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">2018</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:hungCompactingPickingGrowing2019">Compacting &amp;
Picking</a></td>
<td>2019</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="citation"
data-cites="hungCompactingPickingGrowing2019">Hung et al. (<a
href="#ref-hungCompactingPickingGrowing2019"
role="doc-biblioref">2019</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="#sec:gongEfficientTrainingBERT2019">Progressive
Stacking</a></td>
<td>2019</td>
<td>Accelerate pre-training</td>
<td>Fixed schedule</td>
<td>Duplicated layers added on top</td>
<td>Duplication of existing layers</td>
<td><span class="citation"
data-cites="gongEfficientTrainingBERT2019">Gong et al. (<a
href="#ref-gongEfficientTrainingBERT2019"
role="doc-biblioref">2019</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:wuFirefly2021">Firefly</a></td>
<td>2020</td>
<td>NAS and CL</td>
<td>Fixed Schedule</td>
<td>Decided based on gradient information</td>
<td>Function-preserving transforms</td>
<td><span class="citation" data-cites="wuFirefly2021">Wu et al. (<a
href="#ref-wuFirefly2021" role="doc-biblioref">2020</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="#sec:hassantabarSCANNSynthesisCompact2021">SCANN</a></td>
<td>2021</td>
<td>NAS</td>
<td>Fixed schedule</td>
<td>Connections based on gradient, Neurons based on activation</td>
<td>Connections based on gradient, Neurons are split</td>
<td><span class="citation"
data-cites="hassantabarSCANNSynthesisCompact2021">Hassantabar, Wang, and
Jha (<a href="#ref-hassantabarSCANNSynthesisCompact2021"
role="doc-biblioref">2021</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:evciGradMaxGrowingNeural2022">GradMax</a></td>
<td>2022</td>
<td>NAS</td>
<td>Fixed Schedule</td>
<td>Fixed (GradMax could be adapted for this decision)</td>
<td>By maximizing the gradient of new parts using SVD</td>
<td><span class="citation"
data-cites="evciGradMaxGrowingNeural2022">Evci et al. (<a
href="#ref-evciGradMaxGrowingNeural2022"
role="doc-biblioref">2022</a>)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h1 id="sec:summaries">Summaries of the Reviewed Publications</h1>
<p>The following sections give short summaries of each of the
publications which we deemed relevant.</p>
<h2 id="sec:chenNet2NetAcceleratingLearning2016"><a
href="http://arxiv.org/abs/1511.05641">Net2Net: Accelerating Learning
via Knowledge Transfer</a> <span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">(<a
href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">Chen, Goodfellow, and Shlens 2016</a>)</span></h2>
<p><span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span> introduce the idea of training a
larger student network from an existing smaller teacher network by using
<em>function-preserving transformations</em>. These transformations
(<em>Net2Net</em> operations) allow the rapid transfer of learned
knowledge and omits the need to retrain the larger network from
scratch.</p>
<p>The authors propose two operations two increase the student network’s
size:</p>
<ol type="1">
<li>Growing in width: adding more units in each hidden layer and</li>
<li>growing in depth: adding more hidden layers.</li>
</ol>
<p>Growth along the <strong>width</strong> dimension is achieved by
randomly splitting the original neurons (<em>Net2WiderNet</em>
operation, see Figure <a href="#fig:splitting_neuron">1</a>). Input
weights of new neurons are copied from existing and the output weight of
existing neurons is equally distributed among all copies (the old neuron
and all new copies).</p>
<p>If no dropout is used, <span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span> propose to add a small noise to
the input weights to break the symmetry.</p>
<div id="fig:splitting_neuron" class="fignos">
<figure>
<img src="img/splitting_neuron.svg" style="width:8cm"
alt="Figure 1: Illustration of Net2WiderNet. Here, a single neuron is split in two parts. However, multiple neurons can be split in one operation and each neuron may be split in multiple parts" />
<figcaption aria-hidden="true"><span>Figure 1:</span> Illustration of
<em>Net2WiderNet</em>. Here, a single neuron is split in two parts.
However, multiple neurons can be split in one operation and each neuron
may be split in multiple parts</figcaption>
</figure>
</div>
<p>Growth along the depth dimension is achieved by adding new layers
which are initialized with the identity function. This requires
idempotent activation functions: the activation function <span
class="math inline">\(\phi\)</span> needs to chosen such that <span
class="math inline">\(\phi(\mathbf{I}\phi(\mathbf{v}))\)</span> for any
vector <span class="math inline">\(\mathbf{v}\)</span>. For rectified
linear units (ReLU) this is the case, for some the identity matrix may
be replace with a different matrix, in some cases it may not be as easy
to construct an identity layer.</p>
<p>The experiments are conducted on an Inception network architecture
<span class="citation"
data-cites="szegedyGoingDeeperConvolutions2014">(<a
href="#ref-szegedyGoingDeeperConvolutions2014"
role="doc-biblioref">Szegedy et al. 2014</a>)</span>, a convolutional
neural network (CNN). They show that rapid transfer of knowledge through
the two types of network transformations is possible, allowing the
faster exploration of model families contained in this architecture
space.</p>
<h2 id="sec:weiNetworkMorphism2016"><a
href="http://arxiv.org/abs/1603.01670">Network Morphism</a> <span
class="citation" data-cites="weiNetworkMorphism2016">(<a
href="#ref-weiNetworkMorphism2016" role="doc-biblioref">Wei et al.
2016</a>)</span></h2>
<p><span class="citation" data-cites="weiNetworkMorphism2016">Wei et al.
(<a href="#ref-weiNetworkMorphism2016"
role="doc-biblioref">2016</a>)</span> follow a very similar path to
<span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span>: <em>function-preserving
transformations</em> are used to grow a parent (or “teacher”) network to
a child (or “student”) network while maintaining the same function.</p>
<p><span class="citation" data-cites="weiNetworkMorphism2016">Wei et al.
(<a href="#ref-weiNetworkMorphism2016"
role="doc-biblioref">2016</a>)</span> point out that using an identity
layer for growing in depth (which they refer to as “IdMorp”) may be
sub-optimal as it is extremely sparse. Additionally, they reiterate the
requirement of idempotent activation functions, which they deem
insufficient.</p>
<p>Through an iterative procedure, a convolutional layer is decomposed
into two layers, retaining a large number of non-zero entries.</p>
<p><span class="citation" data-cites="weiStableNetworkMorphism2019">Wei,
Wang, and Chen (<a href="#ref-weiStableNetworkMorphism2019"
role="doc-biblioref">2019</a>)</span> further improve the decomposition
method in order to minimize the performance drop after transforming
(growing) the network.</p>
<p>Instead of relying on idempotent activation functions, <span
class="citation" data-cites="weiNetworkMorphism2016">Wei et al. (<a
href="#ref-weiNetworkMorphism2016" role="doc-biblioref">2016</a>)</span>
introduce parametric activation functions for new layers: A parameter
<span class="math inline">\(a\)</span> interpolates between the identity
function and the non-linear activation function. <span
class="math inline">\(a\)</span> is initialized with one such that there
is essentially no activation function. Over the course of future
training, the parameter can be learned to make the activation function
non-linear [for an example see Figure <a href="#fig:ptanh">2</a> or the
parametric rectified activation units (PReLU), <span class="citation"
data-cites="heDelvingDeepRectifiers2015">He et al. (<a
href="#ref-heDelvingDeepRectifiers2015"
role="doc-biblioref">2015</a>)</span>].</p>
<div id="fig:ptanh" class="fignos">
<figure>
<img src="img/parametric_tanh.svg"
alt="Figure 2: Illustration of an parametric tanh function: with a=1 the function is equal to the identity function, with a=0 it is equal to tanh." />
<figcaption aria-hidden="true"><span>Figure 2:</span> Illustration of an
parametric tanh function: with <span class="math inline">\(a=1\)</span>
the function is equal to the identity function, with <span
class="math inline">\(a=0\)</span> it is equal to tanh.</figcaption>
</figure>
</div>
<h2 id="sec:rusuProgressiveNeuralNetworks2016"><a
href="http://arxiv.org/abs/1606.04671">Progressive Neural Networks</a>
<span class="citation"
data-cites="rusuProgressiveNeuralNetworks2016">(<a
href="#ref-rusuProgressiveNeuralNetworks2016" role="doc-biblioref">Rusu
et al. 2016</a>)</span></h2>
<p><span class="citation"
data-cites="rusuProgressiveNeuralNetworks2016">Rusu et al. (<a
href="#ref-rusuProgressiveNeuralNetworks2016"
role="doc-biblioref">2016</a>)</span> develop <em>Progressive
Networks</em> for tackling catastrophic forgetting. The idea is to grow
networks when learning new tasks. The older parts of the networks are
frozen and their function incorporated using adapters to allow for
knowledge transfer from earlier tasks. Each time a new tasks is learned,
the network is further extended (a new column is added).</p>
<div id="fig:progressive_nn_columns" class="fignos">
<figure>
<img src="img/progressive_nn_columns.svg" style="width:10cm"
alt="Figure 3: Figure from Rusu et al. (2016) illustrating the use of columns and adapters." />
<figcaption aria-hidden="true"><span>Figure 3:</span> Figure from <span
class="citation" data-cites="rusuProgressiveNeuralNetworks2016">Rusu et
al. (<a href="#ref-rusuProgressiveNeuralNetworks2016"
role="doc-biblioref">2016</a>)</span> illustrating the use of columns
and adapters.</figcaption>
</figure>
</div>
<p>During inference (as well as during training), a task identifier is
needed to select the column which matches the current task. By freezing
the older parts of the networks during training, the performance on
tasks learned in early training is guaranteed to remain stable, as the
respective weights (and therefore the models function) cannot
change.</p>
<h2 id="sec:caiEfficientArchitectureSearch2017"><a
href="http://arxiv.org/abs/1707.04873">Efficient Architecture Search by
Network Transformation</a> <span class="citation"
data-cites="caiEfficientArchitectureSearch2017">(<a
href="#ref-caiEfficientArchitectureSearch2017" role="doc-biblioref">Cai
et al. 2017</a>)</span></h2>
<p><span class="citation"
data-cites="caiEfficientArchitectureSearch2017">Cai et al. (<a
href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span> propose using a reinforcement
learning (RL) agent as a meta-controller in order to decide when and
where the network is grown (using function-preserving
transformations).</p>
<p>By using variable-length strings <span class="citation"
data-cites="zophNeuralArchitectureSearch2017">(see <a
href="#ref-zophNeuralArchitectureSearch2017" role="doc-biblioref">Zoph
and Le 2017</a>)</span> to represent the network architecture, an RL
agent can be used to generate a function-preserving transformation <span
class="citation" data-cites="chenNet2NetAcceleratingLearning2016">(<a
href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">Chen, Goodfellow, and Shlens 2016</a>)</span>.</p>
<p>The network architecture is encoded using an bidirectional LSTM and
the encoding is then fed to a number of actor networks which decides
whether and where transformations should be applied. For each possible
network transformation there is one actor network. For an illustration,
see Figure <a href="#fig:cail_rl_agent">4</a>.</p>
<div id="fig:cail_rl_agent" class="fignos">
<figure>
<img src="img/cai_rl_agent.svg" style="width:100.0%"
alt="Figure 4: Illustration of the architecture embedding. Figure from Cai et al. (2017)" />
<figcaption aria-hidden="true"><span>Figure 4:</span> Illustration of
the architecture embedding. Figure from <span class="citation"
data-cites="caiEfficientArchitectureSearch2017">Cai et al. (<a
href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span></figcaption>
</figure>
</div>
<p>In each growth phase, 10 networks are sampled from the
meta-controller and trained for 20 epochs (on image datasets CIFAR-10
and SVHN). Based on the accuracy on held out validation data (<span
class="math inline">\(acc_v\)</span>), a reward for the meta-controller
is calculated. Instead of directly using the accuracy as reward signal,
<span class="citation"
data-cites="caiEfficientArchitectureSearch2017">Cai et al. (<a
href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span> propose using a non-linear
transformation in order to increase the reward if the accuracy is
already high (an increase of 1% starting at 90% is more difficult than
starting at 60%):</p>
<p><span class="math display">\[
\tan(acc_v \times \frac{\pi}{2})
\]</span></p>
<h2 id="sec:elskenSimpleEfficientArchitecture2017"><a
href="http://arxiv.org/abs/1711.04528">Simple And Efficient Architecture
Search for Convolutional Neural Networks</a> <span class="citation"
data-cites="elskenSimpleEfficientArchitecture2017">(<a
href="#ref-elskenSimpleEfficientArchitecture2017"
role="doc-biblioref">Elsken, Metzen, and Hutter 2017</a>)</span></h2>
<p><span class="citation"
data-cites="elskenSimpleEfficientArchitecture2017">Elsken, Metzen, and
Hutter (<a href="#ref-elskenSimpleEfficientArchitecture2017"
role="doc-biblioref">2017</a>)</span> propose an iterative NAS algorithm
(Neural Architecture Search by Hillclimbing; short: NASH) which – in
each growth step – produces a set of grown child networks (using
function-preserving transformations). Each child is trained for a small
number of epochs before the most promising candidate is chosen. This
best performing child is then used for repeating the procedure (see
fig. <a href="#fig:nash">5</a>).</p>
<div id="fig:nash" class="fignos">
<figure>
<img src="img/nash.svg" style="width:100.0%"
alt="Figure 5: Illustration of the NASH algorithm. A set of children is grown and trained. Then the best candidate is chosen. Figure from Elsken, Metzen, and Hutter (2017)." />
<figcaption aria-hidden="true"><span>Figure 5:</span> Illustration of
the NASH algorithm. A set of children is grown and trained. Then the
best candidate is chosen. Figure from <span class="citation"
data-cites="elskenSimpleEfficientArchitecture2017">Elsken, Metzen, and
Hutter (<a href="#ref-elskenSimpleEfficientArchitecture2017"
role="doc-biblioref">2017</a>)</span>.</figcaption>
</figure>
</div>
<p>Additionally, they use a different set of network morphisms (or
<em>function-preserving transformations</em>) such as an interpolating
layer <span class="citation"
data-cites="weiNetworkMorphism2016">(similar to the parametric
activation functions in <a href="#ref-weiNetworkMorphism2016"
role="doc-biblioref">Wei et al. 2016</a>)</span>: Here an existing layer
is replaced by an affine combination of the existing layer and some new
ones (starting with all weights of the new layers being 0, and the
weight of the existing layer to be 1).</p>
<h2 id="sec:daiNeSTNeuralNetwork2018"><a
href="http://arxiv.org/abs/1711.02017">NeST: A Neural Network Synthesis
Tool Based on a Grow-and-Prune Paradigm</a> <span class="citation"
data-cites="daiNeSTNeuralNetwork2018">(<a
href="#ref-daiNeSTNeuralNetwork2018" role="doc-biblioref">Dai, Yin, and
Jha 2018</a>)</span></h2>
<p><span class="citation" data-cites="daiNeSTNeuralNetwork2018">Dai,
Yin, and Jha (<a href="#ref-daiNeSTNeuralNetwork2018"
role="doc-biblioref">2018</a>)</span> utilize growth with network
architecture search (NAS) in mind. They note that trial-and-error
approaches are inefficient as a process and can (as a product) lead to
inefficient architectures which might far more parameters than required.
To combat these issues, they propose NeST, which trains weights as well
as the architecture.</p>
<div id="fig:nest" class="fignos">
<figure>
<img src="img/nest.svg" style="width:100.0%"
alt="Figure 6: Illustration of the steps for synthesizing an architecture using NeST (figure from Dai, Yin, and Jha 2018)." />
<figcaption aria-hidden="true"><span>Figure 6:</span> Illustration of
the steps for synthesizing an architecture using NeST <span
class="citation" data-cites="daiNeSTNeuralNetwork2018">(figure from <a
href="#ref-daiNeSTNeuralNetwork2018" role="doc-biblioref">Dai, Yin, and
Jha 2018</a>)</span>.</figcaption>
</figure>
</div>
<p>NeST starts with an initial small network (a <em>seed
architecture</em>). In a first phase, the network is grown by adding new
connections based on their gradient (assuming they already existed with
an weight of 0), and growing new neurons in a layer <span
class="math inline">\(l\)</span> in order to connect existing neurons
<span class="math inline">\(n\)</span> and <span
class="math inline">\(m\)</span> in layers <span
class="math inline">\(l-1\)</span> and <span
class="math inline">\(l+1\)</span> which if they were connected
directly, exhibited a large gradient magnitude:</p>
<p><span class="math display">\[
G_{m,n} = \frac{\partial L}{\partial u_m^{l+1}} x_n^{l-1} \ge threshold
\]</span></p>
<p>Here, <span class="math inline">\(u_m^{l+1}\)</span> is the sum of
incoming activations of neuron <span class="math inline">\(m\)</span> in
layer <span class="math inline">\(l+1\)</span> and <span
class="math inline">\(x_n^{l-1}\)</span> is the activation of neuron
<span class="math inline">\(n\)</span> in layer <span
class="math inline">\(l+1\)</span>. The threshold is calculated using a
growth proportion.</p>
<p>In a second phase, weights are iteratively pruned. Between each
pruning step, the network is retrained to recover its performance.</p>
<p>It should be noted however, that NeST does not grow additional layers
(nor does it remove layers) and hence is limited to a fixed number of
layers.</p>
<h2 id="sec:caiPathLevelNetworkTransformation2018"><a
href="https://proceedings.mlr.press/v80/cai18a.html">Path-Level Network
Transformation for Efficient Architecture Search</a> <span
class="citation" data-cites="caiPathLevelNetworkTransformation2018">(<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">Cai et al. 2018</a>)</span></h2>
<p>This publication offers an incremental extension to enable branched
architectures using function-preserving transformations <span
class="citation" data-cites="chenNet2NetAcceleratingLearning2016">(<a
href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">Chen, Goodfellow, and Shlens 2016</a>)</span> and
growing the model using a RL agent based meta-controller as in <span
class="citation" data-cites="caiEfficientArchitectureSearch2017">Cai et
al. (<a href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span>.</p>
<p><span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">Cai et al. (<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">2018</a>)</span> propose <em>path-level</em>
transformations which allows the branching of neural networks (whereas
<span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span> initially proposed just growing
deeper and wider). Instead of restricting the architecture space to
sequences of layers, <span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">Cai et al. (<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">2018</a>)</span> represent their network
architecture as trees.</p>
<div id="fig:path_level" class="fignos">
<figure>
<img src="img/path_level.svg" style="width:100.0%"
alt="Figure 7: Illustration of a series of network transformations. The last part shows the tree-structure of the transformation. Figure from Cai et al. (2018). " />
<figcaption aria-hidden="true"><span>Figure 7:</span> Illustration of a
series of network transformations. The last part shows the
tree-structure of the transformation. Figure from <span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">Cai et al. (<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">2018</a>)</span>. </figcaption>
</figure>
</div>
<p>Each <em>path-level</em> transformation follows either an
<em>add</em> or a <em>concatenation</em> merge scheme. In the
<em>add</em> scheme, a layer is replaced by two copies and each of their
outputs is multiplied by 0.5. This is similar to splitting a neuron,
except on a layer level. Transformation (a) in Figure <a
href="#fig:path_level">7</a> shows such a transformation.</p>
<p>In the <em>concatenation</em> scheme (step (b) in Figure <a
href="#fig:path_level">7</a>), the outputs dimensions (in a fully
connected layer: neuron outputs, in a convolutional layer: output
channels, etc.) are split among the different branches and the output of
each branch is later concatenated. This introduces branches while
preserving the function and each branch is unique.</p>
<p>These two schemes do not introduce a significant change to the
network. However, in combination with the existing operations <span
class="citation" data-cites="chenNet2NetAcceleratingLearning2016">(in <a
href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">Chen, Goodfellow, and Shlens 2016</a>)</span>, this
can lead to a variety of branched architectures.</p>
<h2 id="sec:hungCompactingPickingGrowing2019"><a
href="http://arxiv.org/abs/1910.06562">Compacting, Picking and Growing
for Unforgetting Continual Learning</a> <span class="citation"
data-cites="hungCompactingPickingGrowing2019">(<a
href="#ref-hungCompactingPickingGrowing2019" role="doc-biblioref">Hung
et al. 2019</a>)</span></h2>
<p><em>coming soon</em></p>
<div id="fig:compacting_picking_growing" class="fignos">
<figure>
<img src="img/compacting_picking_growing.svg" style="width:100.0%"
alt="Figure 8: " />
<figcaption aria-hidden="true"><span>Figure 8:</span> </figcaption>
</figure>
</div>
<h2 id="sec:gongEfficientTrainingBERT2019"><a
href="https://proceedings.mlr.press/v97/gong19a.html">Efficient Training
of BERT by Progressively Stacking</a> <span class="citation"
data-cites="gongEfficientTrainingBERT2019">(<a
href="#ref-gongEfficientTrainingBERT2019" role="doc-biblioref">Gong et
al. 2019</a>)</span></h2>
<p><span class="citation"
data-cites="gongEfficientTrainingBERT2019">Gong et al. (<a
href="#ref-gongEfficientTrainingBERT2019"
role="doc-biblioref">2019</a>)</span> observe, that self-attention
distributions across different layers of well-trained BERT model
typically exhibit a large degree of similarity. Hence, they propose
starting the pre-training of BERT models with a smaller number (3) of
hidden layers. Over the course of the training, these pre-trained layers
are duplicated twice (and added on top, see Figure <a
href="#fig:progressive_stacking">9</a>) and trained between each
stacking operations in order to differentiate the layers.</p>
<p>By training with few layers for a large portion of the pre-training,
<span class="citation" data-cites="gongEfficientTrainingBERT2019">Gong
et al. (<a href="#ref-gongEfficientTrainingBERT2019"
role="doc-biblioref">2019</a>)</span> can reduce the pre-training time
by <span class="math inline">\(\sim 35\%\)</span> with only a small loss
of performance.</p>
<div id="fig:progressive_stacking" class="fignos">
<figure>
<img src="img/progressive_stacking.svg" style="height:5cm"
alt="Figure 9: In each stacking step, the number of layers is doubled (Gong et al. 2019)." />
<figcaption aria-hidden="true"><span>Figure 9:</span> In each stacking
step, the number of layers is doubled <span class="citation"
data-cites="gongEfficientTrainingBERT2019">(<a
href="#ref-gongEfficientTrainingBERT2019" role="doc-biblioref">Gong et
al. 2019</a>)</span>.</figcaption>
</figure>
</div>
<h2 id="sec:wuFirefly2021"><a
href="https://proceedings.neurips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf">Firefly
Neural Architecture Descent: A General Approach for Growing Neural
Networks</a> <span class="citation" data-cites="wuFirefly2021">(<a
href="#ref-wuFirefly2021" role="doc-biblioref">Wu et al.
2020</a>)</span></h2>
<p><span class="citation" data-cites="wuFirefly2021">Wu et al. (<a
href="#ref-wuFirefly2021" role="doc-biblioref">2020</a>)</span> propose
alternating between training and growth steps. In each growth step, the
network is grown to be wider and deeper. During each growth step,
multiple candidate elements (neurons or layers) are temporarily added to
the network. The contribution of each candidate part is multiplied with
some step size <span class="math inline">\(\epsilon\)</span> to maintain
the original function. By using the training data (or some portion of
it), one can then calculate how beneficial these new parts might be
during future training. <span class="citation"
data-cites="wuFirefly2021">Wu et al. (<a href="#ref-wuFirefly2021"
role="doc-biblioref">2020</a>)</span> show that by using Taylor
approximation, this reduces to looking a the gradients of these new
parts.</p>
<p>Additionally, <span class="citation" data-cites="wuFirefly2021">Wu et
al. (<a href="#ref-wuFirefly2021" role="doc-biblioref">2020</a>)</span>
test their approach in a CL task-incremental setting. For each task, a
neuron mask is created (which can be retrieved using the available task
identifier). This allows the model to share structure while maintaining
its function on old tasks and hence to maintain a good average accuracy
even after multiple tasks have been learned.</p>
<div id="fig:firefly_new_neurons" class="fignos">
<figure>
<img src="img/firefly_new_neurons.svg" style="width:100.0%"
alt="Figure 10: Figure from Wu et al. (2020) illustrating how new neurons can be added." />
<figcaption aria-hidden="true"><span>Figure 10:</span> Figure from <span
class="citation" data-cites="wuFirefly2021">Wu et al. (<a
href="#ref-wuFirefly2021" role="doc-biblioref">2020</a>)</span>
illustrating how new neurons can be added.</figcaption>
</figure>
</div>
<h2 id="sec:hassantabarSCANNSynthesisCompact2021">SCANN: Synthesis of
Compact and Accurate Neural Networks <span class="citation"
data-cites="hassantabarSCANNSynthesisCompact2021">(<a
href="#ref-hassantabarSCANNSynthesisCompact2021"
role="doc-biblioref">Hassantabar, Wang, and Jha 2021</a>)</span></h2>
<p><span class="citation"
data-cites="hassantabarSCANNSynthesisCompact2021">Hassantabar, Wang, and
Jha (<a href="#ref-hassantabarSCANNSynthesisCompact2021"
role="doc-biblioref">2021</a>)</span> develop SCANN to produce compact
and accurate feed-forward networks (FFNNs). In this paper, they aim to
improve on prior work (namely: <a
href="#sec:daiNeSTNeuralNetwork2018">NeST</a>) and allow the method to
grow in depth as well.</p>
<p>SCANN includes three operations to modify the network
architecture:</p>
<ol type="1">
<li>Growing new connections,</li>
<li>growing new neurons, and</li>
<li>pruning existing connections.</li>
</ol>
<p>New connections are grown based on the gradient magnitude they would
exhibit (if they were present). This follows the proposal of <span
class="citation" data-cites="daiNeSTNeuralNetwork2018">Dai, Yin, and Jha
(<a href="#ref-daiNeSTNeuralNetwork2018"
role="doc-biblioref">2018</a>)</span> for <a
href="#sec:daiNeSTNeuralNetwork2018">NeST</a>.</p>
<p>New neurons are grown based on the activations existing neurons
exhibit when training data is passed through the network: Neurons with
large activation magnitude are selected for splitting.</p>
<p>Finally, connections which have small weight magnitudes are pruned.
This is similar to <a href="#sec:daiNeSTNeuralNetwork2018">NeST</a> as
well.</p>
<p><span class="citation"
data-cites="hassantabarSCANNSynthesisCompact2021">Hassantabar, Wang, and
Jha (<a href="#ref-hassantabarSCANNSynthesisCompact2021"
role="doc-biblioref">2021</a>)</span> describe three training schemes
with different configurations of network modification operations:
Different orders of executing the operations as well as different
degrees of growth and pruning and different sizes of initial
networks.</p>
<p>They show that all of these three training schemes can yield well
performing models with different numbers of parameters.</p>
<h2 id="sec:evciGradMaxGrowingNeural2022"><a
href="http://arxiv.org/abs/2201.05125">GradMax: Growing Neural Networks
Using Gradient Information</a> <span class="citation"
data-cites="evciGradMaxGrowingNeural2022">(<a
href="#ref-evciGradMaxGrowingNeural2022" role="doc-biblioref">Evci et
al. 2022</a>)</span></h2>
<p><span class="citation" data-cites="evciGradMaxGrowingNeural2022">Evci
et al. (<a href="#ref-evciGradMaxGrowingNeural2022"
role="doc-biblioref">2022</a>)</span> focus on the question
<strong>how</strong> new neurons are initialized. They propose
initializing new neurons such that the gradient norm of new weights are
maximized while maintaining the models function. By enforcing large
gradient norms of the new weights, the objective function is guaranteed
to decrease in the next step of gradient descent.</p>
<p>When using a step size of <span
class="math inline">\(\frac{1}{\beta}\)</span> on a function with a
<span class="math inline">\(\beta\)</span>-Lipschitz gradient, the loss
is upper-bounded by:</p>
<p><span class="math display">\[
L(W_{new}) \le L(W) - \frac{\beta}{2} \| \nabla L (W) \|^2
\]</span></p>
<p>While a constant Lipschitz constant generally does not necessarily
exist in neural networks the authors use this as a motivation to assume
that large gradient norms will lead to large decreases in the loss
function after the next</p>
<p>In GradMax, the maximum gradient norms (with some constraint) are
approximated using singular value decomposition (SVD). The authors
additionally provide experiments using optimization to produce large
gradient norms instead of using the closed-form solution of SVD. While
they find that SVD usually produces better results, it can only be used,
if the activation function returns 0 given an input of 0.</p>
<p>The authors note that this idea could also be utilized to select
<strong>where</strong> new neurons should be grown. The decision where
to add new neurons could be made by looking at the singular values (e,g,
selecting the largest or adding a neuron, once the singular value
reaches a threshold). This idea is very similar to the strategy of <span
class="citation" data-cites="wuFirefly2021">Wu et al. (<a
href="#ref-wuFirefly2021" role="doc-biblioref">2020</a>)</span> which
use a very similar technique to choose <strong>where</strong> to grow
neurons (but use a different initialization strategy).</p>
<h1 class="unnumbered" id="sec:references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-boineeAutomaticClassificationUsing2003" class="csl-entry"
role="doc-biblioentry">
Boinee, P., A. De Angelis, and E. Milotti. 2003. <span>“Automatic
<span>Classification</span> Using <span>Self-Organising Neural
Networks</span> in <span>Astrophysical Experiments</span>.”</span>
<em>arXiv:cs/0307031</em>, July. <a
href="http://arxiv.org/abs/cs/0307031">http://arxiv.org/abs/cs/0307031</a>.
</div>
<div id="ref-caiEfficientArchitectureSearch2017" class="csl-entry"
role="doc-biblioentry">
Cai, Han, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. 2017.
<span>“Efficient <span>Architecture Search</span> by <span>Network
Transformation</span>.”</span> <em>arXiv:1707.04873 [Cs]</em>, November.
<a
href="http://arxiv.org/abs/1707.04873">http://arxiv.org/abs/1707.04873</a>.
</div>
<div id="ref-caiPathLevelNetworkTransformation2018" class="csl-entry"
role="doc-biblioentry">
Cai, Han, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. 2018.
<span>“Path-<span>Level Network Transformation</span> for
<span>Efficient Architecture Search</span>.”</span> In <em>Proceedings
of the 35th <span>International Conference</span> on <span>Machine
Learning</span></em>, 678–87. <span>PMLR</span>. <a
href="https://proceedings.mlr.press/v80/cai18a.html">https://proceedings.mlr.press/v80/cai18a.html</a>.
</div>
<div id="ref-chenNet2NetAcceleratingLearning2016" class="csl-entry"
role="doc-biblioentry">
Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. 2016.
<span>“<span>Net2Net</span>: <span>Accelerating Learning</span> via
<span>Knowledge Transfer</span>.”</span> <em>arXiv:1511.05641 [Cs]</em>,
April. <a
href="http://arxiv.org/abs/1511.05641">http://arxiv.org/abs/1511.05641</a>.
</div>
<div id="ref-daiNeSTNeuralNetwork2018" class="csl-entry"
role="doc-biblioentry">
Dai, Xiaoliang, Hongxu Yin, and Niraj K. Jha. 2018.
<span>“<span>NeST</span>: <span>A Neural Network Synthesis Tool
Based</span> on a <span class="nocase">Grow-and-Prune
Paradigm</span>,”</span> no. arXiv:1711.02017 (June). <a
href="https://doi.org/10.48550/arXiv.1711.02017">https://doi.org/10.48550/arXiv.1711.02017</a>.
</div>
<div id="ref-elskenSimpleEfficientArchitecture2017" class="csl-entry"
role="doc-biblioentry">
Elsken, Thomas, Jan-Hendrik Metzen, and Frank Hutter. 2017.
<span>“Simple <span>And Efficient Architecture Search</span> for
<span>Convolutional Neural Networks</span>.”</span> <span>arXiv</span>.
<a
href="http://arxiv.org/abs/1711.04528">http://arxiv.org/abs/1711.04528</a>.
</div>
<div id="ref-evciGradMaxGrowingNeural2022" class="csl-entry"
role="doc-biblioentry">
Evci, Utku, Bart van Merriënboer, Thomas Unterthiner, Max Vladymyrov,
and Fabian Pedregosa. 2022. <span>“<span>GradMax</span>: <span>Growing
Neural Networks</span> Using <span>Gradient Information</span>.”</span>
<span>arXiv</span>. <a
href="https://doi.org/10.48550/arXiv.2201.05125">https://doi.org/10.48550/arXiv.2201.05125</a>.
</div>
<div id="ref-fritzkeGrowingNeuralGas1994" class="csl-entry"
role="doc-biblioentry">
Fritzke, Bernd. 1994. <span>“A <span>Growing Neural Gas Network Learns
Topologies</span>.”</span> <em>NIPS</em>.
</div>
<div id="ref-gongEfficientTrainingBERT2019" class="csl-entry"
role="doc-biblioentry">
Gong, Linyuan, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu.
2019. <span>“Efficient <span>Training</span> of <span>BERT</span> by
<span>Progressively Stacking</span>.”</span> In <em>Proceedings of the
36th <span>International Conference</span> on <span>Machine
Learning</span></em>, 2337–46. <span>PMLR</span>. <a
href="https://proceedings.mlr.press/v97/gong19a.html">https://proceedings.mlr.press/v97/gong19a.html</a>.
</div>
<div id="ref-hassantabarSCANNSynthesisCompact2021" class="csl-entry"
role="doc-biblioentry">
Hassantabar, Shayan, Zeyu Wang, and Niraj K. Jha. 2021.
<span>“<span>SCANN</span>: <span>Synthesis</span> of
<span>Compact</span> and <span>Accurate Neural Networks</span>.”</span>
<em>IEEE Transactions on Computer-Aided Design of Integrated Circuits
and Systems</em> 41 (9): 3012–25. <a
href="https://doi.org/10.1109/TCAD.2021.3116470">https://doi.org/10.1109/TCAD.2021.3116470</a>.
</div>
<div id="ref-heDelvingDeepRectifiers2015" class="csl-entry"
role="doc-biblioentry">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.
<span>“Delving <span>Deep</span> into <span>Rectifiers</span>:
<span>Surpassing Human-Level Performance</span> on <span>ImageNet
Classification</span>.”</span> In <em>2015 <span>IEEE International
Conference</span> on <span>Computer Vision</span>
(<span>ICCV</span>)</em>, 1026–34. <span>Santiago, Chile</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICCV.2015.123">https://doi.org/10.1109/ICCV.2015.123</a>.
</div>
<div id="ref-hungCompactingPickingGrowing2019" class="csl-entry"
role="doc-biblioentry">
Hung, Steven C. Y., Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming
Chan, and Chu-Song Chen. 2019. <span>“Compacting, <span>Picking</span>
and <span>Growing</span> for <span>Unforgetting Continual
Learning</span>.”</span> <em>arXiv:1910.06562 [Cs, Stat]</em>, October.
<a
href="http://arxiv.org/abs/1910.06562">http://arxiv.org/abs/1910.06562</a>.
</div>
<div id="ref-palomoLearningTopologiesGrowing2016" class="csl-entry"
role="doc-biblioentry">
Palomo, E. J., and Ezequiel López-Rubio. 2016. <span>“Learning
<span>Topologies</span> with the <span>Growing Neural
Forest</span>.”</span> <em>Int. J. Neural Syst.</em> <a
href="https://doi.org/10.1142/S0129065716500192">https://doi.org/10.1142/S0129065716500192</a>.
</div>
<div id="ref-piastraGrowingSelfOrganizingNetwork2009" class="csl-entry"
role="doc-biblioentry">
Piastra, Marco. 2009. <span>“A <span>Growing Self-Organizing
Network</span> for <span>Reconstructing Curves</span> and
<span>Surfaces</span>.”</span> <em>2009 International Joint Conference
on Neural Networks</em>, June, 2533–40. <a
href="https://doi.org/10.1109/IJCNN.2009.5178709">https://doi.org/10.1109/IJCNN.2009.5178709</a>.
</div>
<div id="ref-rusuProgressiveNeuralNetworks2016" class="csl-entry"
role="doc-biblioentry">
Rusu, Andrei A., Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer,
James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
2016. <span>“Progressive <span>Neural Networks</span>.”</span>
<em>arXiv:1606.04671 [Cs]</em>, September. <a
href="http://arxiv.org/abs/1606.04671">http://arxiv.org/abs/1606.04671</a>.
</div>
<div id="ref-szegedyGoingDeeperConvolutions2014" class="csl-entry"
role="doc-biblioentry">
Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
Rabinovich. 2014. <span>“Going <span>Deeper</span> with
<span>Convolutions</span>.”</span> <span>arXiv</span>. <a
href="https://doi.org/10.48550/arXiv.1409.4842">https://doi.org/10.48550/arXiv.1409.4842</a>.
</div>
<div id="ref-weiStableNetworkMorphism2019" class="csl-entry"
role="doc-biblioentry">
Wei, Tao, Changhu Wang, and Chang Wen Chen. 2019. <span>“Stable
<span>Network Morphism</span>.”</span> In <em>2019 <span>International
Joint Conference</span> on <span>Neural Networks</span>
(<span>IJCNN</span>)</em>, 1–8. <a
href="https://doi.org/10.1109/IJCNN.2019.8851955">https://doi.org/10.1109/IJCNN.2019.8851955</a>.
</div>
<div id="ref-weiNetworkMorphism2016" class="csl-entry"
role="doc-biblioentry">
Wei, Tao, Changhu Wang, Yong Rui, and Chang Wen Chen. 2016.
<span>“Network <span>Morphism</span>.”</span> <em>arXiv:1603.01670
[Cs]</em>, March. <a
href="http://arxiv.org/abs/1603.01670">http://arxiv.org/abs/1603.01670</a>.
</div>
<div id="ref-wuFirefly2021" class="csl-entry" role="doc-biblioentry">
Wu, Lemeng, Bo Liu, Peter Stone, and Qiang Liu. 2020. <span>“Firefly
Neural Architecture Descent: A General Approach for Growing Neural
Networks.”</span> In <em>Advances in Neural Information Processing
Systems</em>, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, 33:22373–83. <span>Curran Associates, Inc.</span> <a
href="https://proceedings.neurips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf</a>.
</div>
<div id="ref-zophNeuralArchitectureSearch2017" class="csl-entry"
role="doc-biblioentry">
Zoph, Barret, and Quoc V. Le. 2017. <span>“Neural <span>Architecture
Search</span> with <span>Reinforcement Learning</span>.”</span>
<span>arXiv</span>. <a
href="https://doi.org/10.48550/arXiv.1611.01578">https://doi.org/10.48550/arXiv.1611.01578</a>.
</div>
</div>

<script>
function scroll_to_top() {
  document.body.scrollTop = 0; // For Safari
  document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
}

var scroll_top_button = document.querySelector('#scroll-to-top')

scroll_top_button.addEventListener('click', scroll_to_top)

window.onscroll = () => {
  if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 20) {
    scroll_top_button.style.opacity = 0.9;
  } else {
    scroll_top_button.style.opacity = 0;
  }
};
</script>

</body>
</html>
