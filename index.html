<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Review of Literature on Growing Neural Networks</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="https://fonts.xz.style/serve/inter.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css">
  <link rel="stylesheet" href="style.css">
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>

<script src="https://kit.fontawesome.com/81529ac3d0.js" crossorigin="anonymous"></script>
<script src="sortable_table.js" crossorigin="anonymous"></script>

</head>
<body>
<header class="section">

<div class="container" style="float: right;">
<a href="growing_review.pdf">
  <button class="pdf-button"><i class="far fa-file-pdf"></i> View as PDF</button>
</a>

<a href="https://github.com/plonerma/growing-review">
  <button class="github-button"><i class="fab fa-github"></i> Edit on Github</button>
</a>
</div>
<div class="container">


<h1 class="title">Review of Literature on Growing Neural Networks</h1>


<div>

<div class="author-container">
<p class="author"><a href="https://maxploner.de">Max Ploner</a></p>
<p class="affilation"><a href="https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en">HU
Berlin</a></p>
<p class="affilation"><a href="https://www.scienceofintelligence.de/">Science
of Intelligence</a></p>
</div>
</div>

</div>
</header>


<nav id="TOC">
  <input id="toc-collapsible" class="toc-toggle" type="checkbox">
  <label for="toc-collapsible" class="toc-toggle-label">Contents</label>
  <div class="toc-content">
    <div class="toc-content-inner">
<ul>
<li><a href="#sec:introduction"
id="toc-sec:introduction">Introduction</a></li>
<li><a href="#sec:categorization"
id="toc-sec:categorization">Categorization</a></li>
<li><a href="#sec:summaries" id="toc-sec:summaries">Summaries of the
Reviewed Publications</a>
<ul>
<li><a href="#sec:chenNet2NetAcceleratingLearning2016"
id="toc-sec:chenNet2NetAcceleratingLearning2016">Net2Net: Accelerating
Learning via Knowledge Transfer <span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">(Chen, Goodfellow, and
Shlens 2016)</span></a></li>
<li><a href="#sec:weiNetworkMorphism2016"
id="toc-sec:weiNetworkMorphism2016">Network Morphism <span
class="citation" data-cites="weiNetworkMorphism2016">(Wei et al.
2016)</span></a></li>
<li><a href="#sec:rusuProgressiveNeuralNetworks2016"
id="toc-sec:rusuProgressiveNeuralNetworks2016">Progressive Neural
Networks <span class="citation"
data-cites="rusuProgressiveNeuralNetworks2016">(Rusu et al.
2016)</span></a></li>
<li><a href="#sec:caiEfficientArchitectureSearch2017"
id="toc-sec:caiEfficientArchitectureSearch2017">Efficient Architecture
Search by Network Transformation <span class="citation"
data-cites="caiEfficientArchitectureSearch2017">(Cai et al.
2017)</span></a></li>
<li><a href="#sec:daiNeSTNeuralNetwork2018"
id="toc-sec:daiNeSTNeuralNetwork2018">NeST: A Neural Network Synthesis
Tool Based on a Grow-and-Prune Paradigm <span class="citation"
data-cites="daiNeSTNeuralNetwork2018">(Dai, Yin, and Jha
2018)</span></a></li>
<li><a href="#sec:caiPathLevelNetworkTransformation2018"
id="toc-sec:caiPathLevelNetworkTransformation2018">Path-Level Network
Transformation for Efficient Architecture Search <span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">(Cai et al.
2018)</span></a></li>
<li><a href="#sec:hungCompactingPickingGrowing2019"
id="toc-sec:hungCompactingPickingGrowing2019">Compacting, Picking and
Growing for Unforgetting Continual Learning <span class="citation"
data-cites="hungCompactingPickingGrowing2019">(Hung et al.
2019)</span></a></li>
<li><a href="#sec:wuFirefly2021" id="toc-sec:wuFirefly2021">Firefly
Neural Architecture Descent: A General Approach for Growing Neural
Networks <span class="citation" data-cites="wuFirefly2021">(Wu et al.
2020)</span></a></li>
<li><a href="#sec:evciGradMaxGrowingNeural2022"
id="toc-sec:evciGradMaxGrowingNeural2022">GradMax: Growing Neural
Networks Using Gradient Information <span class="citation"
data-cites="evciGradMaxGrowingNeural2022">(Evci et al.
2022)</span></a></li>
</ul></li>
<li><a href="#sec:references"
id="toc-sec:references">References</a></li>
</ul>
    </div>
  </div>
</div>
</nav>



<h1 id="sec:introduction">Introduction</h1>
<p>This article aims to summarize the existing literature concerned with
growing artificial neural networks. For each paper it will list the most
significant contribution. The following four questions will guide the
summary of each paper:</p>
<ol type="1">
<li><strong>Why</strong> are models grown? What is the goal or metric
the approach is evaluated on?</li>
<li><strong>When</strong> are the models grown?</li>
<li><strong>Where</strong> are the models grown?</li>
<li><strong>How</strong> are the the new parts initialized?</li>
</ol>
<p>Each paper tries to make progress in answering at least one of the
questions. Hence, they can be used to categorize these papers.</p>
<h1 id="sec:categorization">Categorization</h1>
<div class="sortable paper-categories">
<div id="tbl:papers_cat" class="tablenos">
<table id="tbl:papers_cat" style="width:100%;">
<caption><span>Table 1:</span> Papers according to the three questions
(see Section <a href="#sec:introduction">1</a>). </caption>
<colgroup>
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th>Short Title</th>
<th>Year</th>
<th>Why?</th>
<th>When?</th>
<th>Where?</th>
<th>How?</th>
<th>Paper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="#sec:chenNet2NetAcceleratingLearning2016">Net2Net</a></td>
<td>2016</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:weiNetworkMorphism2016">Network Morphism</a></td>
<td>2016</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="citation" data-cites="weiNetworkMorphism2016">Wei et
al. (<a href="#ref-weiNetworkMorphism2016"
role="doc-biblioref">2016</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="#sec:rusuProgressiveNeuralNetworks2016">Progressive
Nets</a></td>
<td>2016</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="citation"
data-cites="rusuProgressiveNeuralNetworks2016">Rusu et al. (<a
href="#ref-rusuProgressiveNeuralNetworks2016"
role="doc-biblioref">2016</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:caiEfficientArchitectureSearch2017">NAS using Network
Transformations</a></td>
<td>2017</td>
<td>NAS</td>
<td>Fixed schedule</td>
<td>Decided by RL agent</td>
<td>Function-preserving transformations</td>
<td><span class="citation"
data-cites="caiEfficientArchitectureSearch2017">Cai et al. (<a
href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="#sec:daiNeSTNeuralNetwork2018">NeST</a></td>
<td>2018</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="citation" data-cites="daiNeSTNeuralNetwork2018">Dai,
Yin, and Jha (<a href="#ref-daiNeSTNeuralNetwork2018"
role="doc-biblioref">2018</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:caiPathLevelNetworkTransformation2018">Path-Level
Transformations</a></td>
<td>2018</td>
<td></td>
<td></td>
<td>RL agent</td>
<td></td>
<td><span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">Cai et al. (<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">2018</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="#sec:hungCompactingPickingGrowing2019">Compacting &amp;
Picking</a></td>
<td>2019</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="citation"
data-cites="hungCompactingPickingGrowing2019">Hung et al. (<a
href="#ref-hungCompactingPickingGrowing2019"
role="doc-biblioref">2019</a>)</span></td>
</tr>
<tr class="even">
<td><a href="#sec:wuFirefly2021">Firefly</a></td>
<td>2020</td>
<td></td>
<td>Fixed Schedule</td>
<td></td>
<td></td>
<td><span class="citation" data-cites="wuFirefly2021">Wu et al. (<a
href="#ref-wuFirefly2021" role="doc-biblioref">2020</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="#sec:evciGradMaxGrowingNeural2022">GradMax</a></td>
<td>2022</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="citation"
data-cites="evciGradMaxGrowingNeural2022">Evci et al. (<a
href="#ref-evciGradMaxGrowingNeural2022"
role="doc-biblioref">2022</a>)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h1 id="sec:summaries">Summaries of the Reviewed Publications</h1>
<p>The following sections give short summaries of each of the
publications which we deemed relevant.</p>
<h2 id="sec:chenNet2NetAcceleratingLearning2016"><a
href="http://arxiv.org/abs/1511.05641">Net2Net: Accelerating Learning
via Knowledge Transfer</a> <span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">(<a
href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">Chen, Goodfellow, and Shlens 2016</a>)</span></h2>
<p><span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span> introduce the idea of training a
larger student network from an existing smaller teacher network by using
<em>function-preserving transformations</em>. These transformations
(<em>Net2Net</em> operations) allow the rapid transfer of learned
knowledge and omits the need to retrain the larger network from
scratch.</p>
<p>The authors propose two operations two increase the student network’s
size:</p>
<ol type="1">
<li>Growing in width: adding more units in each hidden layer and</li>
<li>growing in depth: adding more hidden layers.</li>
</ol>
<p>Growth along the <strong>width</strong> dimension is achieved by
randomly splitting the original neurons (<em>Net2WiderNet</em>
operation, see Figure <a href="#fig:splitting_neuron">1</a>). Input
weights of new neurons are copied from existing and the output weight of
existing neurons is equally distributed among all copies (the old neuron
and all new copies).</p>
<p>If no dropout is used, <span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span> propose to add a small noise to
the input weights to break the symmetry.</p>
<div id="fig:splitting_neuron" class="fignos">
<figure>
<img src="img/splitting_neuron.svg" style="width:8cm"
alt="Figure 1: Illustration of Net2WiderNet. Here, a single neuron is split in two parts. However, multiple neurons can be split in one operation and each neuron may be split in multiple parts" />
<figcaption aria-hidden="true"><span>Figure 1:</span> Illustration of
<em>Net2WiderNet</em>. Here, a single neuron is split in two parts.
However, multiple neurons can be split in one operation and each neuron
may be split in multiple parts</figcaption>
</figure>
</div>
<p>Growth along the depth dimension is achieved by adding new layers
which are initialized with the identity function. This requires
idempotent activation functions: the activation function <span
class="math inline">\(\phi\)</span> needs to chosen such that <span
class="math inline">\(\phi(\mathbf{I}\phi(\mathbf{v}))\)</span> for any
vector <span class="math inline">\(\mathbf{v}\)</span>. For rectified
linear units (ReLU) this is the case, for some the identity matrix may
be replace with a different matrix, in some cases it may not be as easy
to construct an identity layer.</p>
<p>The experiments are conducted on an Inception network architecture
<span class="citation"
data-cites="szegedyGoingDeeperConvolutions2014">(<a
href="#ref-szegedyGoingDeeperConvolutions2014"
role="doc-biblioref">Szegedy et al. 2014</a>)</span>, a convolutional
neural network (CNN). They show that rapid transfer of knowledge through
the two types of network transformations is possible, allowing the
faster exploration of model families contained in this architecture
space.</p>
<h2 id="sec:weiNetworkMorphism2016"><a
href="http://arxiv.org/abs/1603.01670">Network Morphism</a> <span
class="citation" data-cites="weiNetworkMorphism2016">(<a
href="#ref-weiNetworkMorphism2016" role="doc-biblioref">Wei et al.
2016</a>)</span></h2>
<p><span class="citation" data-cites="weiNetworkMorphism2016">Wei et al.
(<a href="#ref-weiNetworkMorphism2016"
role="doc-biblioref">2016</a>)</span> follow a very similar path to
<span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span>: <em>function-preserving
transformations</em> are used to grow a parent (or “teacher”) network to
a child (or “student”) network while maintaining the same function.</p>
<p><span class="citation" data-cites="weiNetworkMorphism2016">Wei et al.
(<a href="#ref-weiNetworkMorphism2016"
role="doc-biblioref">2016</a>)</span> point out that using an identity
layer for growing in depth (which they refer to as “IdMorp”) may be
sub-optimal as it is extremely sparse. Additionally, they reiterate the
requirement of idempotent activation functions, which they deem
insufficient.</p>
<p>Through an iterative procedure, a convolutional layer is decomposed
into two layers, retaining a large number of non-zero entries.</p>
<p><span class="citation" data-cites="weiStableNetworkMorphism2019">Wei,
Wang, and Chen (<a href="#ref-weiStableNetworkMorphism2019"
role="doc-biblioref">2019</a>)</span> furhter improve the decomposition
method in order to minimize the performance drop after transforming
(growing) the network.</p>
<p>Instead of relying on idempotent activation functions, <span
class="citation" data-cites="weiNetworkMorphism2016">Wei et al. (<a
href="#ref-weiNetworkMorphism2016" role="doc-biblioref">2016</a>)</span>
introduce parametric activation functions for new layers: A parameter
<span class="math inline">\(a\)</span> interpolates between the identity
function and the non-linear activation function. <span
class="math inline">\(a\)</span> is initialized with one such that there
is essentially no activation function. Over the course of future
training, the parameter can be learned to make the activation function
non-linear [for an example see Figure <a href="#fig:ptanh">2</a> or the
parametric rectified activation units (PReLU), <span class="citation"
data-cites="heDelvingDeepRectifiers2015">He et al. (<a
href="#ref-heDelvingDeepRectifiers2015"
role="doc-biblioref">2015</a>)</span>].</p>
<div id="fig:ptanh" class="fignos">
<figure>
<img src="img/parametric_tanh.svg"
alt="Figure 2: Illustration of an parametric tanh function: with a=1 the function is equal to the identity function, with a=0 it is equal to tanh." />
<figcaption aria-hidden="true"><span>Figure 2:</span> Illustration of an
parametric tanh function: with <span class="math inline">\(a=1\)</span>
the function is equal to the identity function, with <span
class="math inline">\(a=0\)</span> it is equal to tanh.</figcaption>
</figure>
</div>
<h2 id="sec:rusuProgressiveNeuralNetworks2016"><a
href="http://arxiv.org/abs/1606.04671">Progressive Neural Networks</a>
<span class="citation"
data-cites="rusuProgressiveNeuralNetworks2016">(<a
href="#ref-rusuProgressiveNeuralNetworks2016" role="doc-biblioref">Rusu
et al. 2016</a>)</span></h2>
<p><span class="citation"
data-cites="rusuProgressiveNeuralNetworks2016">Rusu et al. (<a
href="#ref-rusuProgressiveNeuralNetworks2016"
role="doc-biblioref">2016</a>)</span> develop <em>Progressive
Networks</em> for tackling catastrophic forgetting. The idea is to grow
networks when learning new tasks. The older parts of the networks are
frozen and their function incoporated using adapters to allow for
knowledge transfer from earlier tasks. Each time a new tasks is learned,
the network is further extended (a new column is added).</p>
<div id="fig:progressive_nn_columns" class="fignos">
<figure>
<img src="img/progressive_nn_columns.svg" style="width:10cm"
alt="Figure 3: Figure from Rusu et al. (2016) illustrating the use of columns and adapters." />
<figcaption aria-hidden="true"><span>Figure 3:</span> Figure from <span
class="citation" data-cites="rusuProgressiveNeuralNetworks2016">Rusu et
al. (<a href="#ref-rusuProgressiveNeuralNetworks2016"
role="doc-biblioref">2016</a>)</span> illustrating the use of columns
and adapters.</figcaption>
</figure>
</div>
<p>During inference (as well as during training), a task identifier is
needed to select the column which matches the current task. By freezing
the older parts of the networks during training, the performance on
tasks learned in early training is guaranteed to remain stable, as the
respective weights (and therefore the models function) cannot
change.</p>
<h2 id="sec:caiEfficientArchitectureSearch2017"><a
href="http://arxiv.org/abs/1707.04873">Efficient Architecture Search by
Network Transformation</a> <span class="citation"
data-cites="caiEfficientArchitectureSearch2017">(<a
href="#ref-caiEfficientArchitectureSearch2017" role="doc-biblioref">Cai
et al. 2017</a>)</span></h2>
<p><span class="citation"
data-cites="caiEfficientArchitectureSearch2017">Cai et al. (<a
href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span> propose using a reinforcement
learning (RL) agent as a meta-controller in order to decide when and
where the network is grown (using function-preserving
transformations).</p>
<p>By using variable-length strings <span class="citation"
data-cites="zophNeuralArchitectureSearch2017">(see <a
href="#ref-zophNeuralArchitectureSearch2017" role="doc-biblioref">Zoph
and Le 2017</a>)</span> to represent the network architecture, an RL
agent can be used to generate a function-preserving transformation <span
class="citation" data-cites="chenNet2NetAcceleratingLearning2016">(<a
href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">Chen, Goodfellow, and Shlens 2016</a>)</span>.</p>
<p>The network architecture is encoded using an bidirectional LSTM and
the encoding is then fed to a number of actor networks which decides
whether and where transformations should be applied. For each possible
network transformation there is one actor network. For an illustration,
see Figure <a href="#fig:cail_rl_agent">4</a>.</p>
<div id="fig:cail_rl_agent" class="fignos">
<figure>
<img src="img/cai_rl_agent.svg" style="width:100.0%"
alt="Figure 4: Illustration of the architecture embedding. Figure from Cai et al. (2017)" />
<figcaption aria-hidden="true"><span>Figure 4:</span> Illustration of
the architecture embedding. Figure from <span class="citation"
data-cites="caiEfficientArchitectureSearch2017">Cai et al. (<a
href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span></figcaption>
</figure>
</div>
<p>In each growth phase, 10 networks are sampled from the
meta-controller and trained for 20 epochs (on image datasets CIFAR-10
and SVHN). Based on the accuracy on held out validation data (<span
class="math inline">\(acc_v\)</span>), a reward for the meta-controller
is calculated. Instead of directly using the accuracy as reward signal,
<span class="citation"
data-cites="caiEfficientArchitectureSearch2017">Cai et al. (<a
href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span> propose using a non-linear
transformation in order to increase the reward if the accuracy is
already high (an increase of 1% starting at 90% is more difficult than
starting at 60%):</p>
<p><span class="math display">\[
\tan(acc_v \times \frac{\pi}{2})
\]</span></p>
<h2 id="sec:daiNeSTNeuralNetwork2018"><a
href="http://arxiv.org/abs/1711.02017">NeST: A Neural Network Synthesis
Tool Based on a Grow-and-Prune Paradigm</a> <span class="citation"
data-cites="daiNeSTNeuralNetwork2018">(<a
href="#ref-daiNeSTNeuralNetwork2018" role="doc-biblioref">Dai, Yin, and
Jha 2018</a>)</span></h2>
<p><span class="citation" data-cites="daiNeSTNeuralNetwork2018">Dai,
Yin, and Jha (<a href="#ref-daiNeSTNeuralNetwork2018"
role="doc-biblioref">2018</a>)</span> utilize growth with network
architecture search (NAS) in mind. They note that trial-and-error
approaches are inefficient as a process and can (as a product) lead to
inefficient architectures which might far more parameters than required.
To combat these issues, they propose NeST, which trains weights as well
as the architecture.</p>
<div id="fig:nest" class="fignos">
<figure>
<img src="img/nest.svg" style="width:100.0%"
alt="Figure 5: Illustration of the steps for synthesizing an architecture using NeST (figure from Dai, Yin, and Jha 2018)." />
<figcaption aria-hidden="true"><span>Figure 5:</span> Illustration of
the steps for synthesizing an architecture using NeST <span
class="citation" data-cites="daiNeSTNeuralNetwork2018">(figure from <a
href="#ref-daiNeSTNeuralNetwork2018" role="doc-biblioref">Dai, Yin, and
Jha 2018</a>)</span>.</figcaption>
</figure>
</div>
<p>NeST starts with an initial small network (a <em>seed
architecture</em>). In a first phase, the network is grown by adding new
connections based on their gradient (assuming they already existed with
an weight of 0), and growing new neurons in a layer <span
class="math inline">\(l\)</span> in order to connect existing neurons
<span class="math inline">\(n\)</span> and <span
class="math inline">\(m\)</span> in layers <span
class="math inline">\(l-1\)</span> and <span
class="math inline">\(l+1\)</span> which if they were connected
directly, exhibited a large gradient magnitude:</p>
<p><span class="math display">\[
G_{m,n} = \frac{\partial L}{\partial u_m^{l+1}} x_n^{l-1} \ge threshold
\]</span></p>
<p>Here, <span class="math inline">\(u_m^{l+1}\)</span> is the sum of
incoming activiations of neuron <span class="math inline">\(m\)</span>
in layer <span class="math inline">\(l+1\)</span> and <span
class="math inline">\(x_n^{l-1}\)</span> is the activation of neuron
<span class="math inline">\(n\)</span> in layer <span
class="math inline">\(l+1\)</span>. The threshold is calculated using a
growth proportion.</p>
<p>In a second phase, weights are iteratively prruned. Between each
pruning step, the network is retrained to recover its performance.</p>
<h2 id="sec:caiPathLevelNetworkTransformation2018"><a
href="https://proceedings.mlr.press/v80/cai18a.html">Path-Level Network
Transformation for Efficient Architecture Search</a> <span
class="citation" data-cites="caiPathLevelNetworkTransformation2018">(<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">Cai et al. 2018</a>)</span></h2>
<p>This publication offers an incremental extension to enable branched
architectures using function-preserving transformations <span
class="citation" data-cites="chenNet2NetAcceleratingLearning2016">(<a
href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">Chen, Goodfellow, and Shlens 2016</a>)</span> and
growing the model using a RL agent based meta-controller as in <span
class="citation" data-cites="caiEfficientArchitectureSearch2017">Cai et
al. (<a href="#ref-caiEfficientArchitectureSearch2017"
role="doc-biblioref">2017</a>)</span>.</p>
<p><span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">Cai et al. (<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">2018</a>)</span> propose <em>path-level</em>
transformations which allows the branching of neural networks (whereas
<span class="citation"
data-cites="chenNet2NetAcceleratingLearning2016">Chen, Goodfellow, and
Shlens (<a href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">2016</a>)</span> initially proposed just growing
deeper and wider). Instead of restricting the architecture space to
sequences of layers, <span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">Cai et al. (<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">2018</a>)</span> represent their network
architecture as trees.</p>
<div id="fig:path_level" class="fignos">
<figure>
<img src="img/path_level.svg" style="width:100.0%"
alt="Figure 6: Illustration of a series of network transformations. The last part shows the tree-structure of the transformation. Figure from Cai et al. (2018). " />
<figcaption aria-hidden="true"><span>Figure 6:</span> Illustration of a
series of network transformations. The last part shows the
tree-structure of the transformation. Figure from <span class="citation"
data-cites="caiPathLevelNetworkTransformation2018">Cai et al. (<a
href="#ref-caiPathLevelNetworkTransformation2018"
role="doc-biblioref">2018</a>)</span>. </figcaption>
</figure>
</div>
<p>Each <em>path-level</em> transformation follows either an
<em>add</em> or a <em>concatenation</em> merge scheme. In the
<em>add</em> scheme, a layer is replaced by two copies and each of their
outputs is multiplied by 0.5. This is similar to splitting a neuron,
except on a layer level. Transformation (a) in Figure <a
href="#fig:path_level">6</a> shows such a transformation.</p>
<p>In the <em>concatenation</em> scheme (step (b) in Figure <a
href="#fig:path_level">6</a>), the outputs dimensions (in a fully
connected layer: neuron outputs, in a convolutional layer: output
channels, etc.) are split among the different branches and the output of
each branch is later concatenated. This introduces branches while
preserving the function and each branch is unique.</p>
<p>These two schemes do not introduce a significant change to the
network. However, in combination with the existing operations <span
class="citation" data-cites="chenNet2NetAcceleratingLearning2016">(in <a
href="#ref-chenNet2NetAcceleratingLearning2016"
role="doc-biblioref">Chen, Goodfellow, and Shlens 2016</a>)</span>, this
can lead to a variety of branched architectures.</p>
<h2 id="sec:hungCompactingPickingGrowing2019"><a
href="http://arxiv.org/abs/1910.06562">Compacting, Picking and Growing
for Unforgetting Continual Learning</a> <span class="citation"
data-cites="hungCompactingPickingGrowing2019">(<a
href="#ref-hungCompactingPickingGrowing2019" role="doc-biblioref">Hung
et al. 2019</a>)</span></h2>
<div id="fig:compacting_picking_growing" class="fignos">
<figure>
<img src="img/compacting_picking_growing.svg" style="width:100.0%"
alt="Figure 7: " />
<figcaption aria-hidden="true"><span>Figure 7:</span> </figcaption>
</figure>
</div>
<h2 id="sec:wuFirefly2021"><a
href="https://proceedings.neurips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf">Firefly
Neural Architecture Descent: A General Approach for Growing Neural
Networks</a> <span class="citation" data-cites="wuFirefly2021">(<a
href="#ref-wuFirefly2021" role="doc-biblioref">Wu et al.
2020</a>)</span></h2>
<p><em>coming soon</em></p>
<div id="fig:firefly_new_neurons" class="fignos">
<figure>
<img src="img/firefly_new_neurons.svg" style="width:100.0%"
alt="Figure 8: Figure from Wu et al. (2020) illustrating how new neurons can be added." />
<figcaption aria-hidden="true"><span>Figure 8:</span> Figure from <span
class="citation" data-cites="wuFirefly2021">Wu et al. (<a
href="#ref-wuFirefly2021" role="doc-biblioref">2020</a>)</span>
illustrating how new neurons can be added.</figcaption>
</figure>
</div>
<h2 id="sec:evciGradMaxGrowingNeural2022"><a
href="http://arxiv.org/abs/2201.05125">GradMax: Growing Neural Networks
Using Gradient Information</a> <span class="citation"
data-cites="evciGradMaxGrowingNeural2022">(<a
href="#ref-evciGradMaxGrowingNeural2022" role="doc-biblioref">Evci et
al. 2022</a>)</span></h2>
<p><span class="citation" data-cites="evciGradMaxGrowingNeural2022">Evci
et al. (<a href="#ref-evciGradMaxGrowingNeural2022"
role="doc-biblioref">2022</a>)</span> focus on the question
<strong>how</strong> new neurons are initialized. They propose
initializing new neurons such that the gradient norm of new weights are
maximized while maintaining the models function. By enforcing large
gradient norms of the new weights, the objective function is guaranteed
to decrease in the next step of gradient descent.</p>
<p>When using a step size of <span
class="math inline">\(\frac{1}{\beta}\)</span> on a function with a
<span class="math inline">\(\beta\)</span>-Lipschitz gradient, the loss
is upperbounded by:</p>
<p><span class="math display">\[
L(W_{new}) \le L(W) - \frac{\beta}{2} \| \nabla L (W) \|^2
\]</span></p>
<p>While a constant Lipschitz constant generally does not necessarily
exist in neural networks the authors use this as a motivation to assume
that large gradient norms will lead to large decreases in the loss
function after the next</p>
<p>In GradMax, the maximum gradient norms (with some constraint) are
approximated using singular value decomposition (SVD). The authors
additionally provide experiments using optimization to produce large
gradient norms instead of using the closed-form solution of SVD. While
they find that SVD usually produces better results, it can only be used,
if the activation function returns 0 given an input of 0.</p>
<p>The authors note that this idea could also be utilized to select
<strong>where</strong> new neurons should be grown. The decision where
to add new neurons could be made by looking at the singular values (e,g,
selecting the largest or adding a neuron, once the singular value
reaches a threshold). This idea is very similar to the strategy of <span
class="citation" data-cites="wuFirefly2021">Wu et al. (<a
href="#ref-wuFirefly2021" role="doc-biblioref">2020</a>)</span> which
use a very similar technique to choose <strong>where</strong> to grow
neurons (but use a different initialization strategy).</p>
<h1 class="unnumbered" id="sec:references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-caiEfficientArchitectureSearch2017" class="csl-entry"
role="doc-biblioentry">
Cai, Han, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. 2017.
<span>“Efficient <span>Architecture Search</span> by <span>Network
Transformation</span>.”</span> <em>arXiv:1707.04873 [Cs]</em>, November.
<a
href="http://arxiv.org/abs/1707.04873">http://arxiv.org/abs/1707.04873</a>.
</div>
<div id="ref-caiPathLevelNetworkTransformation2018" class="csl-entry"
role="doc-biblioentry">
Cai, Han, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. 2018.
<span>“Path-<span>Level Network Transformation</span> for
<span>Efficient Architecture Search</span>.”</span> In <em>Proceedings
of the 35th <span>International Conference</span> on <span>Machine
Learning</span></em>, 678–87. <span>PMLR</span>. <a
href="https://proceedings.mlr.press/v80/cai18a.html">https://proceedings.mlr.press/v80/cai18a.html</a>.
</div>
<div id="ref-chenNet2NetAcceleratingLearning2016" class="csl-entry"
role="doc-biblioentry">
Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. 2016.
<span>“<span>Net2Net</span>: <span>Accelerating Learning</span> via
<span>Knowledge Transfer</span>.”</span> <em>arXiv:1511.05641 [Cs]</em>,
April. <a
href="http://arxiv.org/abs/1511.05641">http://arxiv.org/abs/1511.05641</a>.
</div>
<div id="ref-daiNeSTNeuralNetwork2018" class="csl-entry"
role="doc-biblioentry">
Dai, Xiaoliang, Hongxu Yin, and Niraj K. Jha. 2018.
<span>“<span>NeST</span>: <span>A Neural Network Synthesis Tool
Based</span> on a <span class="nocase">Grow-and-Prune
Paradigm</span>,”</span> no. arXiv:1711.02017 (June). <a
href="https://doi.org/10.48550/arXiv.1711.02017">https://doi.org/10.48550/arXiv.1711.02017</a>.
</div>
<div id="ref-evciGradMaxGrowingNeural2022" class="csl-entry"
role="doc-biblioentry">
Evci, Utku, Bart van Merriënboer, Thomas Unterthiner, Max Vladymyrov,
and Fabian Pedregosa. 2022. <span>“<span>GradMax</span>: <span>Growing
Neural Networks</span> Using <span>Gradient Information</span>.”</span>
<span>arXiv</span>. <a
href="https://doi.org/10.48550/arXiv.2201.05125">https://doi.org/10.48550/arXiv.2201.05125</a>.
</div>
<div id="ref-heDelvingDeepRectifiers2015" class="csl-entry"
role="doc-biblioentry">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.
<span>“Delving <span>Deep</span> into <span>Rectifiers</span>:
<span>Surpassing Human-Level Performance</span> on <span>ImageNet
Classification</span>.”</span> In <em>2015 <span>IEEE International
Conference</span> on <span>Computer Vision</span>
(<span>ICCV</span>)</em>, 1026–34. <span>Santiago, Chile</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICCV.2015.123">https://doi.org/10.1109/ICCV.2015.123</a>.
</div>
<div id="ref-hungCompactingPickingGrowing2019" class="csl-entry"
role="doc-biblioentry">
Hung, Steven C. Y., Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming
Chan, and Chu-Song Chen. 2019. <span>“Compacting, <span>Picking</span>
and <span>Growing</span> for <span>Unforgetting Continual
Learning</span>.”</span> <em>arXiv:1910.06562 [Cs, Stat]</em>, October.
<a
href="http://arxiv.org/abs/1910.06562">http://arxiv.org/abs/1910.06562</a>.
</div>
<div id="ref-rusuProgressiveNeuralNetworks2016" class="csl-entry"
role="doc-biblioentry">
Rusu, Andrei A., Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer,
James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
2016. <span>“Progressive <span>Neural Networks</span>.”</span>
<em>arXiv:1606.04671 [Cs]</em>, September. <a
href="http://arxiv.org/abs/1606.04671">http://arxiv.org/abs/1606.04671</a>.
</div>
<div id="ref-szegedyGoingDeeperConvolutions2014" class="csl-entry"
role="doc-biblioentry">
Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
Rabinovich. 2014. <span>“Going <span>Deeper</span> with
<span>Convolutions</span>.”</span> <span>arXiv</span>. <a
href="https://doi.org/10.48550/arXiv.1409.4842">https://doi.org/10.48550/arXiv.1409.4842</a>.
</div>
<div id="ref-weiStableNetworkMorphism2019" class="csl-entry"
role="doc-biblioentry">
Wei, Tao, Changhu Wang, and Chang Wen Chen. 2019. <span>“Stable
<span>Network Morphism</span>.”</span> In <em>2019 <span>International
Joint Conference</span> on <span>Neural Networks</span>
(<span>IJCNN</span>)</em>, 1–8. <a
href="https://doi.org/10.1109/IJCNN.2019.8851955">https://doi.org/10.1109/IJCNN.2019.8851955</a>.
</div>
<div id="ref-weiNetworkMorphism2016" class="csl-entry"
role="doc-biblioentry">
Wei, Tao, Changhu Wang, Yong Rui, and Chang Wen Chen. 2016.
<span>“Network <span>Morphism</span>.”</span> <em>arXiv:1603.01670
[Cs]</em>, March. <a
href="http://arxiv.org/abs/1603.01670">http://arxiv.org/abs/1603.01670</a>.
</div>
<div id="ref-wuFirefly2021" class="csl-entry" role="doc-biblioentry">
Wu, Lemeng, Bo Liu, Peter Stone, and Qiang Liu. 2020. <span>“Firefly
Neural Architecture Descent: A General Approach for Growing Neural
Networks.”</span> In <em>Advances in Neural Information Processing
Systems</em>, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, 33:22373–83. <span>Curran Associates, Inc.</span> <a
href="https://proceedings.neurips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf</a>.
</div>
<div id="ref-zophNeuralArchitectureSearch2017" class="csl-entry"
role="doc-biblioentry">
Zoph, Barret, and Quoc V. Le. 2017. <span>“Neural <span>Architecture
Search</span> with <span>Reinforcement Learning</span>.”</span>
<span>arXiv</span>. <a
href="https://doi.org/10.48550/arXiv.1611.01578">https://doi.org/10.48550/arXiv.1611.01578</a>.
</div>
</div>
</body>
</html>
