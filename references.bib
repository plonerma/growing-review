@article{boineeAutomaticClassificationUsing2003,
  title = {Automatic {{Classification}} Using {{Self-Organising Neural Networks}} in {{Astrophysical Experiments}}},
  author = {Boinee, P. and De Angelis, A. and Milotti, E.},
  year = {2003},
  month = jul,
  journal = {arXiv:cs/0307031},
  eprint = {cs/0307031},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/cs/0307031},
  urldate = {2021-05-06},
  abstract = {Self-Organising Maps (SOMs) are effective tools in classification problems, and in recent years the even more powerful Dynamic Growing Neural Networks, a variant of SOMs, have been developed. Automatic Classification (also called clustering) is an important and difficult problem in many Astrophysical experiments, for instance, Gamma Ray Burst classification, or gamma-hadron separation. After a brief introduction to classification problem, we discuss Self-Organising Maps in section 2. Section 3 discusses with various models of growing neural networks and finally in section 4 we discuss the research perspectives in growing neural networks for efficient classification in astrophysical problems.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics,Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,I.5.1,I.5.3}
}

@incollection{caillonGrowingNeuralNetworks2021,
  title = {Growing {{Neural Networks Achieve Flatter Minima}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2021},
  author = {Caillon, Paul and Cerisara, Christophe},
  editor = {Farka{\v s}, Igor and Masulli, Paolo and Otte, Sebastian and Wermter, Stefan},
  year = {2021},
  volume = {12892},
  pages = {222--234},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-86340-1_18},
  url = {https://link.springer.com/10.1007/978-3-030-86340-1_18},
  abstract = {Deep neural networks of sizes commonly encountered in practice are proven to converge towards a global minimum. The flatness of the surface of the loss function in a neighborhood of such minima is often linked with better generalization performances. In this paper, we present a new model of growing neural network in which we incrementally add neurons throughout the learning phase. We study the characteristics of the minima found by such a network compared to those obtained with standard feedforward neural networks. The results of this analysis show that a neural network grown with our procedure converges towards a flatter minimum than a standard neural network with the same number of parameters learned from scratch. Furthermore, our results confirm the link between flatter minima and better generalization performances as the grown models tend to outperform the standard ones. We validate this approach both with small neural networks and with large deep learning models that are state-of-the-art in Natural Language Processing tasks.},
  isbn = {978-3-030-86339-5 978-3-030-86340-1},
  langid = {english}
}

@inproceedings{caiPathLevelNetworkTransformation2018,
  title = {Path-{{Level Network Transformation}} for {{Efficient Architecture Search}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Cai, Han and Yang, Jiacheng and Zhang, Weinan and Han, Song and Yu, Yong},
  year = {2018},
  month = jul,
  pages = {678--687},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/cai18a.html},
  urldate = {2022-10-18},
  abstract = {We introduce a new function-preserving transformation for efficient neural architecture search. This network transformation allows reusing previously trained networks and existing successful architectures that improves sample efficiency. We aim to address the limitation of current network transformation operations that can only perform layer-level architecture modifications, such as adding (pruning) filters or inserting (removing) a layer, which fails to change the topology of connection paths. Our proposed path-level transformation operations enable the meta-controller to modify the path topology of the given network while keeping the merits of reusing weights, and thus allow efficiently designing effective structures with complex path topologies like Inception models. We further propose a bidirectional tree-structured reinforcement learning meta-controller to explore a simple yet highly expressive tree-structured architecture space that can be viewed as a generalization of multi-branch architectures. We experimented on the image classification datasets with limited computational resources (about 200 GPU-hours), where we observed improved parameter efficiency and better test results (97.70\% test accuracy on CIFAR-10 with 14.3M parameters and 74.6\% top-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures.},
  langid = {english}
}

@article{chenNet2NetAcceleratingLearning2016,
  title = {{{Net2Net}}: {{Accelerating Learning}} via {{Knowledge Transfer}}},
  shorttitle = {{{Net2Net}}},
  author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  year = {2016},
  month = apr,
  journal = {arXiv:1511.05641 [cs]},
  eprint = {1511.05641},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1511.05641},
  urldate = {2021-11-15},
  abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{daiIncrementalLearningUsing2019,
  title = {Incremental {{Learning Using}} a {{Grow-and-Prune Paradigm}} with {{Efficient Neural Networks}}},
  author = {Dai, Xiaoliang and Yin, Hongxu and Jha, Niraj K.},
  year = {2019},
  month = may,
  journal = {arXiv:1905.10952 [cs]},
  eprint = {1905.10952},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1905.10952},
  urldate = {2021-05-06},
  abstract = {Deep neural networks (DNNs) have become a widely deployed model for numerous machine learning applications. However, their fixed architecture, substantial training cost, and significant model redundancy make it difficult to efficiently update them to accommodate previously unseen data. To solve these problems, we propose an incremental learning framework based on a grow-and-prune neural network synthesis paradigm. When new data arrive, the neural network first grows new connections based on the gradients to increase the network capacity to accommodate new data. Then, the framework iteratively prunes away connections based on the magnitude of weights to enhance network compactness, and hence recover efficiency. Finally, the model rests at a lightweight DNN that is both ready for inference and suitable for future grow-and-prune updates. The proposed framework improves accuracy, shrinks network size, and significantly reduces the additional training cost for incoming data compared to conventional approaches, such as training from scratch and network fine-tuning. For the LeNet-300-100 and LeNet-5 neural network architectures derived for the MNIST dataset, the framework reduces training cost by up to 64\% (63\%) and 67\% (63\%) compared to training from scratch (network fine-tuning), respectively. For the ResNet-18 architecture derived for the ImageNet dataset and DeepSpeech2 for the AN4 dataset, the corresponding training cost reductions against training from scratch (network fine-tunning) are 64\% (60\%) and 67\% (62\%), respectively. Our derived models contain fewer network parameters but achieve higher accuracy relative to conventional baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{daiNeSTNeuralNetwork2018,
  title = {{{NeST}}: {{A Neural Network Synthesis Tool Based}} on a {{Grow-and-Prune Paradigm}}},
  shorttitle = {{{NeST}}},
  author = {Dai, Xiaoliang and Yin, Hongxu and Jha, Niraj K.},
  year = {2018},
  month = jun,
  number = {arXiv:1711.02017},
  eprint = {1711.02017},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1711.02017},
  url = {http://arxiv.org/abs/1711.02017},
  urldate = {2022-10-17},
  abstract = {Deep neural networks (DNNs) have begun to have a pervasive impact on various applications of machine learning. However, the problem of finding an optimal DNN architecture for large applications is challenging. Common approaches go for deeper and larger DNN architectures but may incur substantial redundancy. To address these problems, we introduce a network growth algorithm that complements network pruning to learn both weights and compact DNN architectures during training. We propose a DNN synthesis tool (NeST) that combines both methods to automate the generation of compact and accurate DNNs. NeST starts with a randomly initialized sparse network called the seed architecture. It iteratively tunes the architecture with gradient-based growth and magnitude-based pruning of neurons and connections. Our experimental results show that NeST yields accurate, yet very compact DNNs, with a wide range of seed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we reduce network parameters by 70.2x (74.3x) and floating-point operations (FLOPs) by 79.4x (43.7x). For the AlexNet and VGG-16 architectures, we reduce network parameters (FLOPs) by 15.7x (4.6x) and 30.2x (8.6x), respectively. NeST's grow-and-prune paradigm delivers significant additional parameter and FLOPs reduction relative to pruning-only methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Reading List}
}

@misc{evciGradMaxGrowingNeural2022,
  title = {{{GradMax}}: {{Growing Neural Networks}} Using {{Gradient Information}}},
  shorttitle = {{{GradMax}}},
  author = {Evci, Utku and {van Merri{\"e}nboer}, Bart and Unterthiner, Thomas and Vladymyrov, Max and Pedregosa, Fabian},
  year = {2022},
  month = feb,
  number = {arXiv:2201.05125},
  eprint = {2201.05125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2201.05125},
  url = {http://arxiv.org/abs/2201.05125},
  urldate = {2022-05-28},
  abstract = {The architecture and the parameters of neural networks are often optimized independently, which requires costly retraining of the parameters whenever the architecture is modified. In this work we instead focus on growing the architecture without requiring costly retraining. We present a method that adds new neurons during training without impacting what is already learned, while improving the training dynamics. We achieve the latter by maximizing the gradients of the new weights and find the optimal initialization efficiently by means of the singular value decomposition (SVD). We call this technique Gradient Maximizing Growth (GradMax) and demonstrate its effectiveness in variety of vision tasks and architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{fritzkeGrowingNeuralGas1994,
  title = {A {{Growing Neural Gas Network Learns Topologies}}},
  author = {Fritzke, Bernd},
  year = {1994},
  journal = {NIPS},
  abstract = {An incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple Hebb-like learning rule. An incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple Hebb-like learning rule. In contrast to previous approaches like the "neural gas" method of Martinetz and Schulten (1991, 1994), this model has no parameters which change over time and is able to continue learning, adding units and connections, until a performance criterion has been met. Applications of the model include vector quantization, clustering, and interpolation.}
}

@inproceedings{gongStacking2019,
  title = {Efficient {{Training}} of {{BERT}} by {{Progressively Stacking}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei and Liu, Tieyan},
  year = {2019},
  pages = {2337--2346},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/gong19a.html},
  abstract = {Unsupervised pre-training is popularly used in natural language processing. By designing proper unsupervised prediction tasks, a deep neural network can be trained and shown to be effective in many downstream tasks. As the data is usually adequate, the model for pre-training is generally huge and contains millions of parameters. Therefore, the training efficiency becomes a critical issue even when using high-performance hardware. In this paper, we explore an efficient training method for the state-of-the-art bidirectional Transformer (BERT) model. By visualizing the self-attention distribution of different layers at different positions in a well-trained BERT model, we find that in most layers, the self-attention distribution will concentrate locally around its position and the start-of-sentence token. Motivating from this, we propose the stacking algorithm to transfer knowledge from a shallow model to a deep model; then we apply stacking progressively to accelerate BERT training. The experimental results showed that the models trained by our training strategy achieve similar performance to models trained from scratch, but our algorithm is much faster.},
  langid = {english}
}

@article{guTransformerGrowthProgressive2021,
  title = {On the {{Transformer Growth}} for {{Progressive BERT Training}}},
  author = {Gu, Xiaotao and Liu, Liyuan and Yu, Hongkun and Li, Jing and Chen, Chen and Han, Jiawei},
  year = {2021},
  journal = {arXiv:2010.12562 [cs]},
  eprint = {2010.12562},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.12562},
  abstract = {Due to the excessive cost of large-scale language model pre-training, considerable efforts have been made to train BERT progressively -- start from an inferior but low-cost model and gradually grow the model to increase the computational complexity. Our objective is to advance the understanding of Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture search, Transformer growth also favors compound scaling. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give operator selection practical guidance. In light of our analyses, the proposed method speeds up BERT pre-training by 73.6\% and 82.2\% for the base and large models respectively, while achieving comparable performances},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{hashimotoJointManyTaskModel2017,
  title = {A {{Joint Many-Task Model}}: {{Growing}} a {{Neural Network}} for {{Multiple NLP Tasks}}},
  shorttitle = {A {{Joint Many-Task Model}}},
  author = {Hashimoto, Kazuma and Xiong, Caiming and Tsuruoka, Yoshimasa and Socher, Richard},
  year = {2017},
  month = jul,
  journal = {arXiv:1611.01587 [cs]},
  eprint = {1611.01587},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.01587},
  urldate = {2021-03-12},
  abstract = {Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Reading List}
}

@article{hassantabarSTEERAGESynthesisNeural2019,
  title = {{{STEERAGE}}: {{Synthesis}} of {{Neural Networks Using Architecture Search}} and {{Grow-and-Prune Methods}}},
  shorttitle = {{{STEERAGE}}},
  author = {Hassantabar, Shayan and Dai, Xiaoliang and Jha, Niraj K.},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.05831 [cs]},
  eprint = {1912.05831},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1912.05831},
  urldate = {2021-03-12},
  abstract = {Neural networks (NNs) have been successfully deployed in various applications of Artificial Intelligence. However, architectural design of these models is still a challenging problem. This is due to the need to navigate a large number of hyperparameters that forces the search space of possible architectures to grow exponentially. Furthermore, using a trial-and-error design approach is very time-consuming and leads to suboptimal architectures. In addition, neural networks are known to have a lot of redundancy. This increases the computational cost of inference and poses a severe obstacle to deployment on Internet-of-Thing (IoT) sensors and edge devices. To address these challenges, we propose the STEERAGE synthesis methodology. It consists of two complementary approaches: intelligent and efficient architecture search, and grow-and-prune NN synthesis. The first step, incorporated in a global search module, uses an accuracy predictor to efficiently navigate the architectural search space. This predictor is built using boosted decision tree regression, iterative sampling, and efficient evolutionary search. This step starts from a base architecture and explores the architecture search space to obtain a variant of the base architecture with the highest performance. The second step involves local search. By taking advantage of various grow-and-prune methodologies for synthesizing convolutional and feed-forward NNs, it not only reduces network redundancy and computational cost, but also boosts model performance. We have evaluated STEERAGE performance on various datasets, including MNIST and CIFAR-10. We demonstrate significant accuracy improvements over the baseline architectures. For the MNIST dataset, our CNN architecture achieves an error rate of 0.66\%, with 8.6\texttimes{} fewer parameters compared to the LeNet-5 baseline. For the CIFAR-10 dataset, we used the ResNet architectures as the baseline. Our STEERAGE-synthesized ResNet-18 has a 2.52\% accuracy improvement over the original ResNet-18, 1.74\% over ResNet-101, and 0.16\% over ResNet-1001, while requiring the number of parameters and floating-point operations comparable to the original ResNet-18. This demonstrates that instead of just increasing the number of layers to increase accuracy, an alternative is to use a better NN architecture with a small number of layers. In addition, STEERAGE achieves an error rate of just 3.86\% with a variant of ResNet architecture with 40 layers. To the best of our knowledge, this is the highest accuracy obtained by ResNet-based architectures on the CIFAR-10 dataset. STEERAGE also obtains the highest accuracy for various other feedforward NNs geared towards edge devices and IoT sensors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Reading List}
}

@article{hungCompactingPickingGrowing2019,
  title = {Compacting, {{Picking}} and {{Growing}} for {{Unforgetting Continual Learning}}},
  author = {Hung, Steven C. Y. and Tu, Cheng-Hao and Wu, Cheng-En and Chen, Chien-Hung and Chan, Yi-Ming and Chen, Chu-Song},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.06562 [cs, stat]},
  eprint = {1910.06562},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.06562},
  urldate = {2021-11-15},
  abstract = {Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning. Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{karrasProgressiveGrowingGANs2018,
  title = {Progressive {{Growing}} of {{GANs}} for {{Improved Quality}}, {{Stability}}, and {{Variation}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  year = {2018},
  month = feb,
  journal = {arXiv:1710.10196 [cs, stat]},
  eprint = {1710.10196},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.10196},
  urldate = {2021-05-06},
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024\^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{kilcherEscapingFlatAreas2018,
  title = {Escaping {{Flat Areas}} via {{Function-Preserving Structural Network Modifications}}},
  author = {Kilcher, Yannic and B{\'e}cigneul, Gary and Hofmann, Thomas},
  year = {2018},
  month = dec,
  url = {https://openreview.net/forum?id=H1eadi0cFQ},
  urldate = {2022-10-18},
  abstract = {Hierarchically embedding smaller networks in larger networks, e.g.\textasciitilde by increasing the number of hidden units, has been studied since the 1990s. The main interest was in understanding possible redundancies in the parameterization, as well as in studying how such embeddings affect critical points. We take these results as a point of departure to devise a novel strategy for escaping from flat regions of the error surface and to address the slow-down of gradient-based methods experienced in plateaus of saddle points. The idea is to expand the dimensionality of a network in a way that guarantees the existence of new escape directions. We call this operation the opening of a tunnel. One may then continue with the larger network either temporarily, i.e.\textasciitilde closing the tunnel later, or permanently, i.e.\textasciitilde iteratively growing the network, whenever needed. We develop our method for fully-connected as well as convolutional layers. Moreover, we present a practical version of our algorithm that requires no network structure modification and can be deployed as plug-and-play into any current deep learning framework. Experimentally, our method shows significant speed-ups.},
  langid = {english}
}

@article{liLearnGrowContinual2019,
  title = {Learn to {{Grow}}: {{A Continual Structure Learning Framework}} for {{Overcoming Catastrophic Forgetting}}},
  author = {Li, Xilai and Zhou, Yingbo and Wu, Tianfu and Socher, Richard and Xiong, Caiming},
  year = {2019},
  pages = {10},
  abstract = {Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting.},
  langid = {english}
}

@article{luCompNetNeuralNetworks2018,
  title = {{{CompNet}}: {{Neural}} Networks Growing via the Compact Network Morphism},
  shorttitle = {{{CompNet}}},
  author = {Lu, Jun and Ma, Wei and Faltings, Boi},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.10316 [cs]},
  eprint = {1804.10316},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1804.10316},
  urldate = {2021-05-06},
  abstract = {It is often the case that the performance of a neural network can be improved by adding layers. In real-world practices, we always train dozens of neural network architectures in parallel which is a wasteful process. We explored \$CompNet\$, in which case we morph a well-trained neural network to a deeper one where network function can be preserved and the added layer is compact. The work of the paper makes two contributions: a). The modified network can converge fast and keep the same functionality so that we do not need to train from scratch again; b). The layer size of the added layer in the neural network is controlled by removing the redundant parameters with sparse optimization. This differs from previous network morphism approaches which tend to add more neurons or channels beyond the actual requirements and result in redundance of the model. The method is illustrated using several neural network structures on different data sets including MNIST and CIFAR10.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing}
}

@misc{maileWhenWhereHow2022,
  title = {When, Where, and How to Add New Neurons to {{ANNs}}},
  author = {Maile, Kaitlin and Rachelson, Emmanuel and Luga, Herv{\'e} and Wilson, Dennis G.},
  year = {2022},
  month = feb,
  number = {arXiv:2202.08539},
  eprint = {2202.08539},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2202.08539},
  url = {http://arxiv.org/abs/2202.08539},
  urldate = {2022-05-28},
  abstract = {Neurogenesis in ANNs is an understudied and difficult problem, even compared to other forms of structural learning like pruning. By decomposing it into triggers and initializations, we introduce a framework for studying the various facets of neurogenesis: when, where, and how to add neurons during the learning process. We present the Neural Orthogonality (NORTH*) suite of neurogenesis strategies, combining layer-wise triggers and initializations based on the orthogonality of activations or weights to dynamically grow performant networks that converge to an efficient size. We evaluate our contributions against other recent neurogenesis works across a variety of supervised learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{mixterGrowingArtificialNeural2020,
  title = {Growing {{Artificial Neural Networks}}},
  author = {Mixter, John and Akoglu, Ali},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.06629 [cs]},
  eprint = {2006.06629},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.06629},
  urldate = {2021-03-12},
  abstract = {Pruning is a legitimate method for reducing the size of a neural network to fit in low SWaP hardware, but the networks must be trained and pruned offline. We propose an algorithm, Artificial Neurogenesis (ANG), that grows rather than prunes the network and enables neural networks to be trained and executed in low SWaP embedded hardware. ANG accomplishes this by using the training data to determine critical connections between layers before the actual training takes place. Our experiments use a modified LeNet-5 as a baseline neural network that achieves a test accuracy of 98.74\% using a total of 61,160 weights. An ANG grown network achieves a test accuracy of 98.80\% with only 21,211 weights.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Reading List}
}

@article{nicosiaGrowingMultiplexNetworks2013,
  title = {Growing Multiplex Networks},
  author = {Nicosia, Vincenzo and Bianconi, Ginestra and Latora, Vito and Barthelemy, Marc},
  year = {2013},
  month = jul,
  journal = {Physical Review Letters},
  volume = {111},
  number = {5},
  eprint = {1302.7126},
  eprinttype = {arxiv},
  pages = {058701},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.111.058701},
  url = {http://arxiv.org/abs/1302.7126},
  urldate = {2021-05-06},
  abstract = {We propose a modeling framework for growing multiplexes where a node can belong to different networks. We define new measures for multiplexes and we identify a number of relevant ingredients for modeling their evolution such as the coupling between the different layers and the arrival time distribution of nodes. The topology of the multiplex changes significantly in the different cases under consideration, with effects of the arrival time of nodes on the degree distribution, average shortest paths and interdependence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Social and Information Networks,Condensed Matter - Disordered Systems and Neural Networks,Physics - Physics and Society}
}

@article{panigrahyMindGrowsCircuits2012,
  title = {The {{Mind Grows Circuits}}},
  author = {Panigrahy, Rina and Zhang, Li},
  year = {2012},
  month = mar,
  journal = {arXiv:1203.0088 [cs]},
  eprint = {1203.0088},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1203.0088},
  urldate = {2021-05-06},
  abstract = {There is a vast supply of prior art that study models for mental processes. Some studies in psychology and philosophy approach it from an inner perspective in terms of experiences and percepts. Others such as neurobiology or connectionist-machines approach it externally by viewing the mind as complex circuit of neurons where each neuron is a primitive binary circuit. In this paper, we also model the mind as a place where a circuit grows, starting as a collection of primitive components at birth and then builds up incrementally in a bottom up fashion. A new node is formed by a simple composition of prior nodes when we undergo a repeated experience that can be described by that composition. Unlike neural networks, however, these circuits take "concepts" or "percepts" as inputs and outputs. Thus the growing circuits can be likened to a growing collection of lambda expressions that are built on top of one another in an attempt to compress the sensory input as a heuristic to bound its Kolmogorov Complexity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Formal Languages and Automata Theory}
}

@article{piastraGrowingSelfOrganizingNetwork2009,
  title = {A {{Growing Self-Organizing Network}} for {{Reconstructing Curves}} and {{Surfaces}}},
  author = {Piastra, Marco},
  year = {2009},
  month = jun,
  journal = {2009 International Joint Conference on Neural Networks},
  eprint = {0812.2969},
  eprinttype = {arxiv},
  pages = {2533--2540},
  doi = {10.1109/IJCNN.2009.5178709},
  url = {http://arxiv.org/abs/0812.2969},
  urldate = {2021-05-06},
  abstract = {Self-organizing networks such as Neural Gas, Growing Neural Gas and many others have been adopted in actual applications for both dimensionality reduction and manifold learning. Typically, in these applications, the structure of the adapted network yields a good estimate of the topology of the unknown subspace from where the input data points are sampled. The approach presented here takes a different perspective, namely by assuming that the input space is a manifold of known dimension. In return, the new type of growing self-organizing network presented gains the ability to adapt itself in way that may guarantee the effective and stable recovery of the exact topological structure of the input manifold.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing}
}

@article{raghavanNeuralNetworksGrown2019,
  title = {Neural Networks Grown and Self-Organized by Noise},
  author = {Raghavan, Guruprasad and Thomson, Matt},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.01039 [nlin, q-bio]},
  eprint = {1906.01039},
  eprinttype = {arxiv},
  primaryclass = {nlin, q-bio},
  url = {http://arxiv.org/abs/1906.01039},
  urldate = {2021-05-06},
  abstract = {Living neural networks emerge through a process of growth and self-organization that begins with a single cell and results in a brain, an organized and functional computational device. Artificial neural networks, however, rely on human-designed, hand-programmed architectures for their remarkable performance. Can we develop artificial computational devices that can grow and self-organize without human intervention? In this paper, we propose a biologically inspired developmental algorithm that can 'grow' a functional, layered neural network from a single initial cell. The algorithm organizes inter-layer connections to construct a convolutional pooling layer, a key constituent of convolutional neural networks (CNN's). Our approach is inspired by the mechanisms employed by the early visual system to wire the retina to the lateral geniculate nucleus (LGN), days before animals open their eyes. The key ingredients for robust self-organization are an emergent spontaneous spatiotemporal activity wave in the first layer and a local learning rule in the second layer that 'learns' the underlying activity pattern in the first layer. The algorithm is adaptable to a wide-range of input-layer geometries, robust to malfunctioning units in the first layer, and so can be used to successfully grow and self-organize pooling architectures of different pool-sizes and shapes. The algorithm provides a primitive procedure for constructing layered neural networks through growth and self-organization. Broadly, our work shows that biologically inspired developmental algorithms can be applied to autonomously grow functional 'brains' in-silico.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Quantitative Biology - Neurons and Cognition}
}

@article{rosenfeldIncrementalLearningDeep2018,
  title = {Incremental {{Learning Through Deep Adaptation}}},
  author = {Rosenfeld, Amir and Tsotsos, John K.},
  year = {2018},
  month = feb,
  journal = {arXiv:1705.04228 [cs]},
  eprint = {1705.04228},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1705.04228},
  urldate = {2021-11-15},
  abstract = {Given an existing trained neural network, it is often desirable to be able to add new capabilities without hindering performance of already learned tasks. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method which fully preserves performance on the original task, with only a small increase (around 20\%) in the number of required parameters while performing on par with more costly finetuning procedures, which typically double the number of parameters. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method and explore different aspects of its behavior.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{rusuProgressiveNeuralNetworks2016,
  title = {Progressive {{Neural Networks}}},
  author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  year = {2016},
  month = sep,
  journal = {arXiv:1606.04671 [cs]},
  eprint = {1606.04671},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1606.04671},
  urldate = {2021-10-14},
  abstract = {Learning to solve complex sequences of tasks\textemdash while both leveraging transfer and avoiding catastrophic forgetting\textemdash remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@article{shaikhLearnBindGrow2020,
  title = {Learn to {{Bind}} and {{Grow Neural Structures}}},
  author = {Shaikh, Azhar and Sinha, Nishant},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.10568 [cs]},
  eprint = {2011.10568},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2011.10568},
  urldate = {2021-05-06},
  abstract = {Task-incremental learning involves the challenging problem of learning new tasks continually, without forgetting past knowledge. Many approaches address the problem by expanding the structure of a shared neural network as tasks arrive, but struggle to grow optimally, without losing past knowledge. We present a new framework, Learn to Bind and Grow, which learns a neural architecture for a new task incrementally, either by binding with layers of a similar task or by expanding layers which are more likely to conflict between tasks. Central to our approach is a novel, interpretable, parameterization of the shared, multi-task architecture space, which then enables computing globally optimal architectures using Bayesian optimization. Experiments on continual learning benchmarks show that our framework performs comparably with earlier expansion based approaches and is able to flexibly compute multiple optimal solutions with performance-size trade-offs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{szegedyGoingDeeperConvolutions2014,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2014},
  month = sep,
  number = {arXiv:1409.4842},
  eprint = {1409.4842},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.4842},
  url = {http://arxiv.org/abs/1409.4842},
  urldate = {2022-10-19},
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{wangEnergyAwareNeuralArchitecture2020,
  title = {Energy-{{Aware Neural Architecture Optimization}} with {{Fast Splitting Steepest Descent}}},
  author = {Wang, Dilin and Li, Meng and Wu, Lemeng and Chandra, Vikas and Liu, Qiang},
  year = {2020},
  month = jul,
  journal = {arXiv:1910.03103 [cs, stat]},
  eprint = {1910.03103},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.03103},
  urldate = {2021-11-22},
  abstract = {Designing energy-efficient networks is of critical importance for enabling state-of-the-art deep learning in mobile and edge settings where the computation and energy budgets are highly limited. Recently, Liu et al. (2019) framed the search of efficient neural architectures into a continuous splitting process: it iteratively splits existing neurons into multiple off-springs to achieve progressive loss minimization, thus finding novel architectures by gradually growing the neural network. However, this method was not specifically tailored for designing energy-efficient networks, and is computationally expensive on large-scale benchmarks. In this work, we substantially improve Liu et al. (2019) in two significant ways: 1) we incorporate the energy cost of splitting different neurons to better guide the splitting process, thereby discovering more energy-efficient network architectures; 2) we substantially speed up the splitting process of Liu et al. (2019), which requires expensive eigen-decomposition, by proposing a highly scalable Rayleigh-quotient stochastic gradient algorithm. Our fast algorithm allows us to reduce the computational cost of splitting to the same level of typical back-propagation updates and enables efficient implementation on GPU. Extensive empirical results show that our method can train highly accurate and energy-efficient networks on challenging datasets such as ImageNet, improving a variety of baselines, including the pruning-based methods and expert-designed architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{weiNetworkMorphism2016,
  title = {Network {{Morphism}}},
  author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.01670 [cs]},
  eprint = {1603.01670},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1603.01670},
  urldate = {2021-05-06},
  abstract = {We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with nonlinearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{wenAutoGrowAutomaticLayer2020,
  title = {{{AutoGrow}}: {{Automatic Layer Growing}} in {{Deep Convolutional Networks}}},
  shorttitle = {{{AutoGrow}}},
  author = {Wen, Wei and Yan, Feng and Chen, Yiran and Li, Hai},
  year = {2020},
  month = jun,
  journal = {arXiv:1906.02909 [cs, stat]},
  eprint = {1906.02909},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.02909},
  urldate = {2021-05-06},
  abstract = {Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN. Our code is available at https://github.com/wenwei202/autogrow.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,Statistics - Machine Learning}
}

@article{wistubaSurveyNeuralArchitecture2019,
  title = {A {{Survey}} on {{Neural Architecture Search}}},
  author = {Wistuba, Martin and Rawat, Ambrish and Pedapati, Tejaswini},
  year = {2019},
  month = jun,
  journal = {arXiv:1905.01392 [cs, stat]},
  eprint = {1905.01392},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.01392},
  urldate = {2021-05-06},
  abstract = {The growing interest in both the automation of machine learning and deep learning has inevitably led to the development of a wide variety of automated methods for neural architecture search. The choice of the network architecture has proven to be critical, and many advances in deep learning spring from its immediate improvements. However, deep learning techniques are computationally intensive and their application requires a high level of domain knowledge. Therefore, even partial automation of this process helps to make deep learning more accessible to both researchers and practitioners. With this survey, we provide a formalism which unifies and categorizes the landscape of existing methods along with a detailed analysis that compares and contrasts the different approaches. We achieve this via a comprehensive discussion of the commonly adopted architecture search spaces and architecture optimization algorithms based on principles of reinforcement learning and evolutionary algorithms along with approaches that incorporate surrogate and one-shot models. Additionally, we address the new research directions which include constrained and multi-objective architecture search as well as automated data augmentation, optimizer and activation function search.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{wuDepthGrowingNeural2019,
  title = {Depth {{Growing}} for {{Neural Machine Translation}}},
  author = {Wu, Lijun and Wang, Yiren and Xia, Yingce and Tian, Fei and Gao, Fei and Qin, Tao and Lai, Jianhuang and Liu, Tie-Yan},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.01968 [cs]},
  eprint = {1907.01968},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1907.01968},
  urldate = {2021-05-06},
  abstract = {While very deep neural networks have shown effectiveness for computer vision and text classification applications, how to increase the network depth of neural machine translation (NMT) models for better translation quality remains a challenging problem. Directly stacking more blocks to the NMT model results in no improvement and even reduces performance. In this work, we propose an effective two-stage approach with three specially designed components to construct deeper NMT models, which result in significant improvements over the strong Transformer baselines on WMT\$14\$ English\$\textbackslash to\$German and English\$\textbackslash to\$French translation tasks\textbackslash footnote\{Our code is available at \textbackslash url\{https://github.com/apeterswu/Depth\_Growing\_NMT\}\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{wuFirefly2021,
  title = {Firefly Neural Architecture Descent: A General Approach for Growing Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wu, Lemeng and Liu, Bo and Stone, Peter and Liu, Qiang},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {22373--22383},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf}
}

@article{wuSteepestDescentNeural2021,
  title = {Steepest {{Descent Neural Architecture Optimization}}: {{Escaping Local Optimum}} with {{Signed Neural Splitting}}},
  shorttitle = {Steepest {{Descent Neural Architecture Optimization}}},
  author = {Wu, Lemeng and Ye, Mao and Lei, Qi and Lee, Jason D. and Liu, Qiang},
  year = {2021},
  month = jun,
  eprint = {2003.10392},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2003.10392},
  url = {http://arxiv.org/abs/2003.10392},
  urldate = {2022-05-28},
  abstract = {Developing efficient and principled neural architecture optimization methods is a critical challenge of modern deep learning. Recently, Liu et al.[19] proposed a splitting steepest descent (S2D) method that jointly optimizes the neural parameters and architectures based on progressively growing network structures by splitting neurons into multiple copies in a steepest descent fashion. However, S2D suffers from a local optimality issue when all the neurons become "splitting stable", a concept akin to local stability in parametric optimization. In this work, we develop a significant and surprising extension of the splitting descent framework that addresses the local optimality issue. The idea is to observe that the original S2D is unnecessarily restricted to splitting neurons into positive weighted copies. By simply allowing both positive and negative weights during splitting, we can eliminate the appearance of splitting stability in S2D and hence escape the local optima to obtain better performance. By incorporating signed splittings, we significantly extend the optimization power of splitting steepest descent both theoretically and empirically. We verify our method on various challenging benchmarks such as CIFAR-100, ImageNet and ModelNet40, on which we outperform S2D and other advanced methods on learning accurate and energy-efficient neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

