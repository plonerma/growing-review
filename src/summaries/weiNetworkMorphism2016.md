@weiNetworkMorphism2016 follow a very similar path to @chenNet2NetAcceleratingLearning2016:
*function-preserving transformations* are used to grow a parent
(or "teacher") network to a child (or "student") network while maintaining
the same function.

@weiNetworkMorphism2016 point out that using an identity layer for growing in
depth (which they refer to as "IdMorp") may be sub-optimal as it is extremely
sparse. Additionally, they reiterate the requirement of idempotent activation
functions, which they deem insufficient.
